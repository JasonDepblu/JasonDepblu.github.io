---
layout: post
title: "Byte Latent Transformer: Patches Scale Better Than Tokens â€” å¼€åˆ›æ— tokenizerè¯­è¨€å»ºæ¨¡çš„æ–°ç»´åº¦"
date: 2025-01-06
reading_time: 30 min
author: Jason Deng
categories: [LLM, papers]
tags: [new, post]
excerpt: â€œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‘å±•å†ç¨‹ä¼ ç»Ÿä¸Šç”±åŸºäºtokenizerçš„æ¶æ„ä¸»å¯¼ã€‚ç„¶è€Œï¼Œå­—èŠ‚æ½œåœ¨å˜æ¢å™¨ï¼ˆBLTï¼‰çš„å¼•å…¥æ ‡å¿—ç€ä¸€ç§èŒƒå¼çš„è½¬å˜ã€‚ç”±Metaçš„ç ”ç©¶äººå‘˜åŠå…¶åˆä½œä¼™ä¼´æå‡ºï¼ŒBLTé€šè¿‡æ¶ˆé™¤æ ‡è®°åŒ–å¹¶å¤„ç†åŸå§‹å­—èŠ‚æ•°æ®ï¼Œåˆ›æ–°æ€§åœ°æé«˜äº†æ•ˆç‡å’Œé²æ£’æ€§ï¼ŒåŒæ—¶åœ¨æ€§èƒ½ä¸Šä¸tokenizer based modelç›¸æŠ—è¡¡ã€‚æœ¬æ–‡å°†æ¢è®¨å…¶è®¾è®¡ã€åŠŸèƒ½åŠå¯¹æœªæ¥è¯­è¨€å»ºæ¨¡çš„å½±å“ã€‚â€

---

### è®ºæ–‡ä¿¡æ¯

<details class="toggle-header">

**Paper URLï¼š** [https://arxiv.org/html/2412.09871v1](https://arxiv.org/html/2412.09871v1)

**Authorï¼š** Artidoro Pagnoni Ram Pasunuru Pedro Rodriguez Benjamin Muller Margaret Li Chunting Zhou Lili Yu Jason Weston Luke Zettlemoyer Gargi Ghosh Mike Lewis Ari Holtzman Srinivasan Iyer

**Codeï¼š** [https://github.com/facebookresearch/blt](https://github.com/facebookresearch/blt)

**organizationï¼š** FAIR at Meta, Paul G. Allen School of Computer Science & Engineering, University of Washington, University of Chicago

\contribution[â€¡]Joint second author, \contribution[â€ ]Joint last author, \contribution[â‹„]Work done at Meta

</details>

---

### æ–‡ç« ç›®å½•

<details class="toggle-header">

1 Introduction

2 Patching: From Individual Bytes to Groups of Bytes

2.1 Strided Patching Every K Bytes

2.2 Space Patching

2.3 Entropy Patching: Using Next-Byte Entropies from a Small Byte LM

2.4 The Byte-Pair Encoding (BPE) Tokenizer and Incremental Patching

3 BLT Architecture

3.1 Latent Global Transformer Model

3.2 Local Encoder

3.3 Local Decoder

4 Experimental Setup

4.1 Pre-training Datasets

4.2 Entropy Model

4.3 Entropy Threshold and Equalizing Context Length

4.4 Entropy Model Context

4.5 FLOPs Estimation

4.6 Bits-Per-Byte Estimation

4.7 Transformer Architecture Hyperparameters

4.8 BLT-Specific Hyperparameters

5 Scaling Trends

5.1 Parameter Matched Compute Optimal Scaling Trends

5.2 Beyond Compute Optimal Task Evaluations

5.3 Patches Scale Better Than Tokens

6 Byte Modeling Improves Robustness

6.1 Character-Level Tasks

6.2 Training BLT from Llama 3

7 Ablations and Discussion

8 Related Work

9 Limitations and Future Work

10 Conclusion

</details>

---

### Tokenizer-freeçš„ç›¸å…³ç ”ç©¶å·¥ä½œ
<details class="toggle-header">

**the key works on tokenizer-free approaches**

| Year  | Model               | Key Features                                                                                     |
| 2022  | ByT5 (Xue et al.)   | - ç¬¬ä¸€ä¸ªå¤§è§„æ¨¡å­—èŠ‚åˆ°å­—èŠ‚æ¨¡å‹ï¼Œæ— éœ€ tokenization <br> - è¡¨ç°å‡ºå¢å¼ºçš„é²æ£’æ€§ï¼Œä½†éœ€è¦æ˜¾è‘—æ›´å¤šçš„è®¡ç®—èµ„æº <br> - ä¸ä½¿ç”¨ä»»ä½•å½¢å¼çš„ compression/patching |
| 2022  | CANINE (Clark et al.) | - æ— éœ€ tokenization çš„é¢„è®­ç»ƒç¼–ç å™¨ï¼Œç›´æ¥å¤„ç† character sequences <br> - é€šè¿‡ strided convolutions è¿›è¡Œä¸‹é‡‡æ ·ä»¥å‡å°‘ sequence length <br> - å°†ä¸‹é‡‡æ ·ä¸ deep transformer stack ç›¸ç»“åˆ |
| 2023  | MegaByte (Yu et al.) | - ä½¿ç”¨å›ºå®šå¤§å°çš„ byte patches ä½œä¸ºä¸€ç§ compression å½¢å¼ <br> - å°† patched representation ä¸ byte-level decoder ç»“åˆ <br> - åœ¨ 1B è§„æ¨¡ä¸‹å±•ç¤ºäº†ä¸ token models ç›¸ç«äº‰çš„æ€§èƒ½ |
| 2024  | MambaByte (Wang et al.) | - ä½¿ç”¨ Mamba state space model æ¶æ„è¿›è¡Œå­—èŠ‚çº§å»ºæ¨¡ <br> - ä½¿ç”¨å›ºå®šçš„ memory state ä»£æ›¿ attention <br> - åœ¨å¢å¼ºé²æ£’æ€§çš„åŒæ—¶ï¼Œè¡¨ç°å‡ºä¸ subword models ç›¸å½“çš„æ€§èƒ½ |
| 2024  | BLT (Pagnoni et al.) | - åŠ¨æ€çš„åŸºäº entropy çš„å­—èŠ‚ patching <br> - ä¸‰éƒ¨åˆ†æ¶æ„ï¼šlocal encoderã€global transformer å’Œ local decoder <br> - åœ¨æå‡ efficiency çš„åŒæ—¶ï¼ŒåŒ¹é…åŸºäº tokenization çš„æ¨¡å‹æ€§èƒ½ |

è¿™äº›å·¥ä½œçš„å…±åŒä¸»é¢˜åŒ…æ‹¬ï¼š

    - ç›´æ¥å¤„ç†å­—èŠ‚/å­—ç¬¦åºåˆ—ï¼Œæ— éœ€tokenization
    
    - ä½¿ç”¨å„ç§å½¢å¼çš„å‹ç¼©ï¼ˆå›ºå®šè¡¥ä¸ã€ä¸‹é‡‡æ ·ã€çŠ¶æ€ç©ºé—´ï¼‰æ¥é«˜æ•ˆå¤„ç†æ›´é•¿çš„åºåˆ—
    
    - å±•ç¤ºäº†æ¯”åŸºäºtokençš„æ¨¡å‹æ›´å¼ºçš„é²æ£’æ€§
    
    - åœ¨ä¿æŒæ— æ ‡è®°åŒ–ä¼˜åŠ¿çš„åŒæ—¶ï¼ŒåŠªåŠ›åŒ¹é…åŸºäºtokençš„æ¨¡å‹æ€§èƒ½

è¿™ä¸€è¿›å±•è¡¨æ˜ï¼Œåœ¨å¤„ç†å­—èŠ‚çº§å»ºæ¨¡çš„è®¡ç®—æŒ‘æˆ˜æ—¶ï¼Œé€æ­¥æé«˜äº†å¤æ‚æ€§ï¼ŒåŒæ—¶ä¿ç•™äº†é¿å…æ ‡è®°åŒ–çš„ä¼˜åŠ¿ã€‚

</details>

---

### **ä¸ºä»€ä¹ˆTokenizeä¸å†å ä¸»å¯¼åœ°ä½**

Tokenizeä¸€ç›´æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„åŸºçŸ³ï¼Œå°†å­—èŠ‚åˆ†ç»„ä¸ºé™æ€æ ‡è®°ä»¥ä¾¿é«˜æ•ˆå¤„ç†ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•ä¹Ÿå¸¦æ¥äº†æŒ‘æˆ˜ï¼š

    - é¢†åŸŸå’Œæ¨¡æ€æ•æ„Ÿæ€§ï¼š Tokenizeå¸¸å¸¸å¼•å…¥åå·®ï¼Œå½±å“è·¨é¢†åŸŸçš„æ€§èƒ½ã€‚
    
    - å™ªå£°è„†å¼±æ€§ï¼š Tokenizer based modelå¯¹è¾“å…¥å™ªå£°æ•æ„Ÿã€‚
    
    - æ­£å­—æ³•æ— çŸ¥ï¼š æ— æ³•ç›´æ¥è®¿é—®åŸå§‹å­—èŠ‚çº§ç‰¹å¾ï¼Œé™åˆ¶äº†ç»†ç²’åº¦çš„è¯­è¨€ç†è§£ã€‚
    
    - å¤šè¯­è¨€ä¸å…¬æ­£ï¼š å›ºå®šçš„è¯æ±‡è¡¨å¦¨ç¢äº†è·¨è¯­è¨€çš„å…¬å¹³è¡¨ç¤ºã€‚

BLTé€šè¿‡å¤„ç†åŸå§‹å­—èŠ‚ï¼Œå¹¶æ ¹æ®ç†µï¼ˆæ•°æ®å¤æ‚åº¦çš„åº¦é‡ï¼‰åŠ¨æ€åœ°å°†å­—èŠ‚åˆ†ç»„ä¸ºè¡¥ä¸ï¼Œä»è€Œå…‹æœäº†è¿™äº›å±€é™æ€§ã€‚æ­¤æ–¹æ³•ç¡®ä¿äº†è®¡ç®—èµ„æºçš„é«˜æ•ˆåˆ†é…ã€‚


---
<details class="toggle-header">

  <p><strong>åŸæ–‡ï¼šAt scale, the computational cost of a Transformer is dominated by large feed-forward network layers that run on every byte, not the cost of the attention mechanism.</strong></p>

 **é—®é¢˜ï¼š** å¦‚ä½•ç†è§£ï¼Ÿ
 
 **å›å¤ï¼š**

åœ¨è§£é‡Šè¿™ä¸ªé—®é¢˜ä¹‹å‰ï¼Œå…ˆå›é¡¾ä¸€ä¸‹æ ‡å‡† Transformer Block çš„ä¸»è¦ç»„æˆéƒ¨åˆ†ï¼Œä»¥åŠå®ƒä»¬å„è‡ªçš„è®¡ç®—æµç¨‹ï¼š
 
- Self-Attention å­å±‚ï¼ˆMulti-Head Attentionï¼‰
 
- å‰é¦ˆç½‘ç»œå­å±‚ï¼ˆFeedForward Network, FFNï¼‰

åœ¨åˆ†æè®¡ç®—å¤æ‚åº¦æ—¶ï¼Œé€šå¸¸æˆ‘ä»¬ç”¨ä»¥ä¸‹ç¬¦å·æ¥è¡¨ç¤ºï¼š
 
- Lï¼šåºåˆ—é•¿åº¦ï¼ˆsequence lengthï¼‰ã€‚
 
- dï¼šè¯å‘é‡æˆ–è€…éšè—å‘é‡çš„ç»´åº¦ï¼ˆhidden sizeï¼‰ã€‚

åœ¨å…¸å‹çš„ Transformer ç»“æ„ä¸­ï¼Œè¿™ä¸¤éƒ¨åˆ†çš„ä¸»è¦è®¡ç®—é‡åˆ†åˆ«æ˜¯ï¼š
 
- Self-Attention çš„è®¡ç®—å¤æ‚åº¦ï¼š $$O(L^2 \cdot d)$$
 
- å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰çš„è®¡ç®—å¤æ‚åº¦ï¼š $$O(L \cdot d^2)$$
 
ä¸‹é¢åˆ†åˆ«è¯´æ˜è¿™ä¸¤ä¸ªå¤æ‚åº¦ä»å“ªé‡Œæ¥ã€‚
 
 ---
 
**1. Self-Attentionï¼š$$O(L^2 \cdot d)$$**
 
***1.1 ä»¥å•å¤´ä¸ºä¾‹çš„è®¡ç®—åˆ†æ***
 
 ä»¥å•å¤´è‡ªæ³¨æ„åŠ›ï¼ˆSelf-Attentionï¼‰ä¸ºä¾‹ï¼Œå‡è®¾è¾“å…¥æ˜¯ä¸€ä¸ªé•¿åº¦ä¸º Lã€ç»´åº¦ä¸º d çš„åºåˆ— $$\mathbf{X}\in \mathbb{R}^{L\times d}$$ã€‚é€šè¿‡æŠ•å½±çŸ©é˜µåˆ†åˆ«å¾—åˆ° Q, K, Vï¼Œå®ƒä»¬çš„ç»´åº¦ä¾ç„¶æ˜¯ $$L \times d$$ï¼ˆæˆ–è€…æ‹†åˆ†æˆå¤šå¤´åï¼Œæ¯ä¸€å¤´çš„ç»´åº¦æ˜¯ $$d\_{\text{head}} = d/h$$ï¼Œä½†æˆ‘ä»¬è¿™é‡Œå…ˆä»¥ä¸æ‹†åˆ†æˆ–ç­‰æ•ˆåˆå¹¶çš„æ–¹å¼åˆ†æï¼‰ã€‚è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„ä¸»è¦è®¡ç®—åœ¨äºï¼š
 
- è®¡ç®—æ³¨æ„åŠ›å¾—åˆ†
     
    $$
    \mathbf{A} = \text{softmax}\Big(\frac{Q K^T}{\sqrt{d}}\Big)
    $$
 
- è¿™é‡Œ Q å’Œ K çš„ç»´åº¦éƒ½æ˜¯ $$(L \times d)$$ã€‚
 
- $$Q$$ $$K^T$$ çš„çŸ©é˜µä¹˜æ³•ä¼šäº§ç”Ÿä¸€ä¸ª $$L \times L$$ çš„çŸ©é˜µï¼Œè®¡ç®—é‡çº¦ä¸º $$O(L \times d \times L) = O(L^2 d)$$ã€‚
 
- ä¹˜å®Œä¹‹åè¿˜è¦è¿›è¡Œä¸€æ¬¡ softmax æ“ä½œï¼Œä¸è¿‡ç›¸å¯¹äº $$L^2 d$$ çš„çŸ©é˜µä¹˜æ³•ï¼Œsoftmax çš„å¼€é”€é€šå¸¸è®°ä¸º $$O(L^2)$$ æˆ–ç•¥å»å¸¸æ•°å› å­åä¸å½±å“ä¸»é¡¹ï¼Œæ‰€ä»¥ä¸»è¦å¤æ‚åº¦è¿˜æ˜¯æ¥è‡ªçŸ©é˜µä¹˜æ³•ã€‚
 
- å°†æ³¨æ„åŠ›å¾—åˆ†ä¸ V ç›¸ä¹˜

    $$ 
    \mathbf{O} = \mathbf{A} \cdot V \quad (\text{æˆ–ç±»ä¼¼å½¢å¼})
    $$
 
 
- $$\mathbf{A} çš„ç»´åº¦æ˜¯ (L \times L)ï¼ŒV çš„ç»´åº¦æ˜¯ (L \times d)$$
 
- æœ€ç»ˆå¾—åˆ°çš„ç»“æœ $$\mathbf{O} ä¸º (L \times d)$$
 
- è¿™é‡Œçš„å¤æ‚åº¦åŒæ ·æ˜¯ $$O(L \times L \times d) = O(L^2 d)$$

 å› æ­¤ï¼Œå¯¹è‡ªæ³¨æ„åŠ›å±‚è€Œè¨€ï¼Œæ¯ä¸ªå¤´çš„ä¸»è¦è®¡ç®—éƒ½åŒ…å«äº†ä¸€ä¸ª $$L \times L$$ çº§åˆ«çš„æ“ä½œï¼Œå†ä¹˜ä¸Šéšè—ç»´åº¦ dã€‚å¦‚æœæ˜¯å¤šå¤´æ³¨æ„åŠ›ï¼ˆMulti-Head Attentionï¼‰ï¼Œå¤šå¤´å¹¶è¡Œæ—¶æ€»ä½“æ¥è¯´ä¹Ÿç»´æŒåŒé‡çº§å¤æ‚åº¦ï¼Œå¸¸æ•°å€ä¼šæ˜¯â€œå¤´æ•°â€ï¼Œå› æ­¤ä¸šç•Œé€šå¸¸å¼•ç”¨çš„å¤æ‚åº¦ç»“è®ºæ˜¯ï¼š

   $$
   \boxed{ \text{Self-Attention å¤æ‚åº¦} \approx O(L^2 \cdot d) }
   $$
 
 
**2. å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰ï¼š$$O(L \cdot d^2)$$**

***2.1 æ ‡å‡† Transformer ä¸­çš„ FFN ç»“æ„***

 åœ¨æ ‡å‡†çš„ Transformer ä¸­ï¼Œå‰é¦ˆç½‘ç»œå­å±‚é€šå¸¸æ˜¯ä¸¤å±‚å…¨è¿æ¥ç½‘ç»œ $$(Linear/MLP)$$ ï¼Œå¹¶åŒ…å«æ¿€æ´»å‡½æ•°ï¼Œæ¯”å¦‚ï¼š
 
   $$
   \text{FFN}(\mathbf{x}) = \max(0, \mathbf{x}\mathbf{W}_1 + \mathbf{b}_1) \mathbf{W}_2 + \mathbf{b}_2
   $$
 
 å…¶ä¸­ï¼š
 
- $$\mathbf{x}$$ çš„ç»´åº¦æ˜¯ $$(1 \times d)$$ï¼Œæˆ–è€…å¦‚æœæŠŠæ‰¹å’Œåºåˆ—é•¿åº¦éƒ½ç®—ä¸Šå°±æ˜¯ $$(L \times d)$$ã€‚
 
- $$\mathbf{W}_1 $$çš„ç»´åº¦æ˜¯ $$(d \times d_{\text{ff}})$$ï¼Œé€šå¸¸ $$d_{\text{ff}} \approx 4d$$ã€‚
 
- $$\mathbf{W}_2$$ çš„ç»´åº¦æ˜¯ $$(d_{\text{ff}} \times d)$$ã€‚
 
 
 
***2.2 è®¡ç®—é‡æ¥æº***
 
- ç¬¬ä¸€å±‚æŠ•å½± $$\mathbf{x}\mathbf{W}_1$$
 
- ç»´åº¦ï¼š$$(L \times d) \times (d \times d_{\text{ff}}) \rightarrow (L \times d\_{\text{ff}})$$
 
- è®¡ç®—å¤æ‚åº¦ï¼š$$O(L \times d \times d_{\text{ff}})$$ã€‚è‹¥ $$d_{\text{ff}} \approx 4d$$ï¼Œåˆ™è¿™ä¸€é¡¹å¤æ‚åº¦çº¦ä¸º $$O(4 L d^2)$$ï¼Œçœç•¥å¸¸æ•°åä¸º $$O(L d^2)$$ã€‚
 
- ç¬¬äºŒå±‚æŠ•å½±ä¸æ¿€æ´» $$\max(0, \cdot) \mathbf{W}_2$$
 
- ç»´åº¦ï¼š$$(L \times d_{\text{ff}}) \times (d_{\text{ff}} \times d) \rightarrow (L \times d)$$
 
- è®¡ç®—å¤æ‚åº¦ï¼š$$O(L \times d_{\text{ff}} \times d)$$ã€‚åŒæ ·ï¼Œè‹¥ $$d_{\text{ff}} = 4d$$ï¼Œåˆ™æ˜¯ $$O(4 L d^2)$$ï¼Œä¹Ÿæ˜¯ $$O(L d^2)$$ çš„é‡çº§ã€‚
 
æŠŠè¿™ä¸¤ä¸ªçŸ©é˜µä¹˜æ³•ç›¸åŠ ï¼Œä¾ç„¶ä¿æŒåœ¨ $$O(L \cdot d^2)$$ é‡çº§ï¼Œæ‰€ä»¥æˆ‘ä»¬å¸¸è§åœ°å°† FFN å­å±‚çš„è®¡ç®—é‡å†™ä¸ºï¼š
 
   $$
   \boxed{ \text{FFN å¤æ‚åº¦} \approx O(L \cdot d^2) }
   $$
 
 
 **3. æ€»ç»“**
 
- è‡ªæ³¨æ„åŠ›ï¼ˆSelf-Attentionï¼‰ä¸­ $$L^2$$ æ¥è‡ªäºâ€œæ‰€æœ‰ä½ç½®ä¸æ‰€æœ‰ä½ç½®â€ä¹‹é—´çš„ç›¸å…³æ€§è®¡ç®—ï¼Œåœ¨çŸ©é˜µä¹˜æ³•å’Œåç»­çš„åŠ æƒè¿‡ç¨‹ä¸­ï¼Œæ¯ä¸ª token éƒ½è¦ä¸é•¿åº¦ä¸º L çš„åºåˆ—è¿›è¡Œ dot-product è®¡ç®—ï¼Œå› æ­¤å¾—åˆ° $$O(L^2 \cdot d)$$ çš„å¤æ‚åº¦ã€‚

- å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰çš„ $$d^2$$ åˆ™æ¥è‡ªäºå¤§ç»´åº¦æ˜ å°„çš„çŸ©é˜µä¹˜æ³•ã€‚FFN é’ˆå¯¹åºåˆ—ä¸­æ¯ä¸ªä½ç½®å•ç‹¬åšæŠ•å½±ï¼Œä½†æŠ•å½±æ‰€ç”¨çš„çŸ©é˜µå¾€å¾€æ˜¯ $$(d \times 4d)$$ å’Œ $$(4d \times d)$$ çš„çº§åˆ«ï¼Œå› æ­¤æ¯ä¸ª token çš„è®¡ç®—é‡æ˜¯ $$O(d^2)$$ï¼Œå¯¹å…¨åºåˆ—å°±æ˜¯ $$O(L \cdot d^2)$$ã€‚

è¿™å°±æ˜¯åœ¨æ ‡å‡† Transformer ä¸­å¸¸è§çš„
 
$$ 
\text{Self-Attention: } O(L^2 d), \quad 
\text{FeedForward Network: } O(L d^2)
$$
çš„ä¸»è¦åŸå› æ‰€åœ¨ã€‚

</details>

---

### **æ ¸å¿ƒåˆ›æ–°ï¼š Patches Over Tokens**

![Figure 3: Patching schemes group bytes in different ways, each leading to a different number of resulting patches. Since each patch is processed using a large transformer step, the number of patches directly determines the bulk of the compute expended in terms of flops. These schemes group bytes into patches by (a) striding every four bytes (Â§2.1) as in MegaByte (Yu et al., 2023), (b) tokenizing with Byte-Pair Encoding (bpe), in this case the Llama-3 (Dubey et al., 2024) tokenizer, (c & d) entropy-based patching as in this work (Â§2.3), (e) patching on space-bytes (Slagle, 2024), (f) and patching on entropy using a small CNN byte-level model with 2-byte context.](https://arxiv.org/html/2412.09871v1/extracted/6066458/assets/patching_types.png)


BLTå°†patchesä½œä¸ºå…¶ä¸»è¦è®¡ç®—å•å…ƒï¼Œçªç ´äº†fixed-tokenè¯æ±‡è¡¨çš„åšæ³•ã€‚è¿™äº›patchesé€šè¿‡åŸºäºç†µçš„åˆ†å‰²ç”Ÿæˆï¼ŒåŠ¨æ€è°ƒæ•´å…¶å¤§å°å’Œå†…å®¹ï¼Œä»¥é€‚åº”æ•°æ®çš„å¤æ‚æ€§ã€‚è¿™ä¸€åˆ›æ–°å¸¦æ¥äº†å‡ ä¸ªä¼˜åŠ¿ï¼š

1. åŠ¨æ€è®¡ç®—åˆ†é…ï¼š é«˜ç†µåŒºåŸŸå¾—åˆ°æ›´å¤šçš„è®¡ç®—å…³æ³¨ï¼Œä»è€Œä¼˜åŒ–èµ„æºçš„ä½¿ç”¨ã€‚

2. å¯æ‰©å±•æ€§ï¼š BLTé€šè¿‡åŒæ—¶å¢åŠ è¡¥ä¸å’Œæ¨¡å‹çš„å¤§å°ï¼Œè€Œä¸å¢åŠ è®¡ç®—æˆæœ¬ï¼Œå±•ç¤ºäº†ä¼˜äºåŸºäºæ ‡è®°çš„æ¨¡å‹çš„æ‰©å±•è¶‹åŠ¿ã€‚

3. æ•ˆç‡æå‡ï¼š é€šè¿‡å°†æ¨ç†æˆæœ¬å‡å°‘æœ€å¤š50%ï¼ŒBLTåœ¨å›ºå®šçš„è®¡ç®—é¢„ç®—å†…æ”¯æŒæ›´å¤§çš„æ¨¡å‹è§„æ¨¡ã€‚

![https://arxiv.org/html/2412.09871v1/x1.png](https://arxiv.org/html/2412.09871v1/x1.png)

![Figure 1 Scaling trends for fixed inference flop models (fully) trained with varying training budgets. In token-based models, a fixed inference budget determines the model size. In contrast, the BLT architecture provides a new scaling axis allowing simultaneous increases in model and patch size while keeping the same training and inference budget. BLT patch-size (ps) 6 and 8 models quickly overtake scaling trends of bpe Llama 2 and 3. Moving to the larger inference budget makes the larger patch size 8 model more desirable sooner. Both BPE compute-optimal point and crossover point are indicated with vertical lines.](https://arxiv.org/html/2412.09871v1/x2.png)

å›¾ç¤ºæ˜¯å›ºå®šæ¨ç†è®¡ç®—é‡æ—¶æ¨¡å‹çš„ç¼©æ”¾è¶‹åŠ¿ï¼ˆåœ¨ä¸åŒè®­ç»ƒé¢„ç®—ä¸‹å®Œæ•´è®­ç»ƒï¼‰ã€‚
åœ¨åŸºäºtokençš„æ¨¡å‹ä¸­ï¼Œå›ºå®šçš„æ¨ç†é¢„ç®—å†³å®šäº†æ¨¡å‹å¤§å°ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒBLTæ¶æ„æä¾›äº†ä¸€ä¸ªæ–°çš„ç¼©æ”¾ç»´åº¦ï¼Œå…è®¸åœ¨ä¿æŒç›¸åŒè®­ç»ƒå’Œæ¨ç†é¢„ç®—çš„åŒæ—¶åŒæ—¶å¢åŠ æ¨¡å‹å’Œpatchå¤§å°ã€‚

<details class="toggle-header">

 <p><strong>åŸæ–‡ï¼šcritical difference between patches and tokens is that with tokens, the model has no direct access to the underlying byte features.</strong></p>
 
**é—®é¢˜1:** è¿™æ˜¯ä¸æ˜¯é€ æˆç»å…¸çš„9.11>9.8çš„é—®é¢˜æ‰€åœ¨ï¼Ÿ

**å›å¤1.1:** 
 æ ¹æ®æ–‡æœ¬æ‰€è¿°ï¼Œä½¿ç”¨tokensï¼ˆæ ‡è®°ï¼‰çš„æ¨¡å‹æ— æ³•ç›´æ¥è®¿é—®åº•å±‚çš„å­—èŠ‚ç‰¹å¾ï¼Œè¿™æ„å‘³ç€æ¨¡å‹åœ¨å¤„ç†æ•°å­—åºåˆ—æ—¶ç¼ºä¹å¯¹åŸå§‹å­—èŠ‚çº§åˆ«çš„ç†è§£ã€‚è¿™å¯èƒ½å¯¼è‡´åœ¨å¤„ç†å’Œæ¯”è¾ƒç±»ä¼¼"9.11"å’Œ"9.8"è¿™æ ·çš„æ•°å€¼æ—¶å‡ºç°é—®é¢˜ï¼Œå› ä¸ºæ¨¡å‹åªèƒ½åŸºäºé¢„å®šä¹‰çš„tokenæ¥å¤„ç†ï¼Œè€Œä¸èƒ½ç›´æ¥ç†è§£å’Œå¤„ç†åº•å±‚çš„å­—èŠ‚çº§æ•°å€¼è¡¨ç¤ºã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒBLTæ¨¡å‹ä½¿ç”¨patchesç›´æ¥å¤„ç†å­—èŠ‚åºåˆ—ï¼Œå¯ä»¥æ›´å¥½åœ°ç†è§£å’Œå¤„ç†åŸå§‹æ•°æ®çš„ç‰¹å¾ï¼Œè¿™å¯èƒ½æœ‰åŠ©äºæ›´å‡†ç¡®åœ°å¤„ç†æ•°å€¼æ¯”è¾ƒã€‚

**å›å¤1.2:**
æ ¹æ®BLTè®ºæ–‡,æˆ‘è®¤ä¸ºå®ƒå¯ä»¥éƒ¨åˆ†ä½†ä¸èƒ½å®Œå…¨è§£å†³æ•°å­—æ¯”è¾ƒè¿™ç±»é—®é¢˜ã€‚è®©æˆ‘è¯¦ç»†åˆ†æä¸€ä¸‹ï¼š

1. BLTçš„ä¼˜åŠ¿ï¼š
   - ç›´æ¥åœ¨byteçº§åˆ«å»ºæ¨¡,æ¯ä¸ªæ•°å­—ä¼šè¢«ç¼–ç ä¸ºå•ç‹¬çš„byte
   - N-gram embeddingå¯ä»¥æ•è·æ•°å­—åºåˆ—çš„æ¨¡å¼
   - ç›¸æ¯”token-basedæ¨¡å‹,BLTä¸ä¼šæŠŠ"9.11"ä½œä¸ºå•ä¸ªtokenå¤„ç†

2. ä¾ç„¶å­˜åœ¨çš„æŒ‘æˆ˜ï¼š
- æ•°å­¦æ¨ç†æ˜¯é«˜å±‚è¯­ä¹‰é—®é¢˜,ä»…ä»…æ”¹å˜tokenè¡¨ç¤ºå¹¶ä¸èƒ½å®Œå…¨è§£å†³
- æ¨¡å‹ä»ç„¶éœ€è¦å­¦ä¹ æ•°å­—çš„æ¦‚å¿µå’Œæ¯”è¾ƒè¿ç®—
- å¯¹å°æ•°ç‚¹çš„å¤„ç†å¯èƒ½ä»æœ‰æ­§ä¹‰

3. å¯èƒ½çš„æ”¹è¿›æ–¹å‘ï¼š
- åœ¨æ•°å­—åºåˆ—ä¸Šä½¿ç”¨ç‰¹æ®Šçš„patchç­–ç•¥
- ä¸ºæ•°å­—æ¯”è¾ƒæ·»åŠ ä¸“é—¨çš„é¢„è®­ç»ƒä»»åŠ¡
- ç»“åˆç‰¹å®šçš„æ•°å­—è¡¨ç¤ºæ–¹æ³•(å¦‚positional encoding)

æ€»çš„æ¥è¯´ï¼ŒBLTçš„byte-levelå¤„ç†å¯èƒ½æœ‰åŠ©äºæ›´å¥½åœ°ç†è§£æ•°å­—åºåˆ—çš„ç»“æ„ï¼Œä½†è¦çœŸæ­£è§£å†³æ•°å­¦æ¨ç†é—®é¢˜ï¼Œè¿˜éœ€è¦åœ¨æ¨¡å‹æ¶æ„å’Œè®­ç»ƒæ–¹æ³•ä¸Šåšè¿›ä¸€æ­¥çš„æ”¹è¿›ã€‚

**é—®é¢˜2:** åŸºäºCoTæ–¹å¼æ˜¯å¦ç›¸å¯¹äºBLTæ›´æœ‰æ•ˆè§£å†³ä¸Šè¿°é—®é¢˜ï¼Ÿ

**å›å¤2:**
å¦‚æœä¸“é—¨é’ˆå¯¹æ•°å­—æ¯”è¾ƒè¿™ç±»é—®é¢˜ï¼Œæˆ‘è®¤ä¸ºåŸºäºChain of Thought (CoT)çš„å¾®è°ƒæ–¹å¼ä¼šæ›´æœ‰æ•ˆï¼š

1. CoTçš„ä¼˜åŠ¿ï¼š
- ç›´æ¥å¼•å¯¼æ¨¡å‹è¿›è¡Œstep-by-stepæ¨ç†
- å¯ä»¥æ˜¾å¼åˆ†è§£æ•°å­—æ¯”è¾ƒæ­¥éª¤ï¼Œå¦‚ï¼š
  Q: 9.11 > 9.8?   
  A: è®©æˆ‘ä»¬ä¸€æ­¥æ­¥åˆ†æï¼š   
  - å…ˆæ¯”è¾ƒæ•´æ•°éƒ¨åˆ†ï¼š9 = 9   
  - å°æ•°ç¬¬ä¸€ä½æ¯”è¾ƒï¼š1 < 8   
  - å› æ­¤ 9.11 < 9.8   ç­”æ¡ˆï¼šå¦ï¼Œ9.11 < 9.8  
- å¯ä»¥é’ˆå¯¹å„ç§æ•°å­—æ¯”è¾ƒæƒ…å†µè®¾è®¡ä¸“é—¨çš„æç¤ºæ¨¡æ¿

2. BLTçš„å±€é™ï¼š
- è™½ç„¶åœ¨byteçº§åˆ«æœ‰æ›´å¥½çš„è¡¨ç¤º
- ä½†æ²¡æœ‰ç›´æ¥æŒ‡å¯¼æ¨¡å‹å¦‚ä½•è¿›è¡Œæ¨ç†
- éœ€è¦æ¨¡å‹è‡ªå·±å­¦ä¹ æ¨ç†æ­¥éª¤ï¼Œéš¾åº¦æ›´å¤§

3. å…·ä½“å®ç°å»ºè®®ï¼š
- æ„å»ºæ•°å­—æ¯”è¾ƒçš„ä¸“é—¨æ•°æ®é›†
- è®¾è®¡æ¸…æ™°çš„CoTæ¨¡æ¿
- å¯ä»¥ç»“åˆå°‘é‡ç¤ºä¾‹çš„few-shotå­¦ä¹ 
- è€ƒè™‘æ·»åŠ è‡ªæˆ‘éªŒè¯æ­¥éª¤

CoTå¾®è°ƒèƒ½ç›´æ¥é’ˆå¯¹é—®é¢˜æœ¬è´¨ï¼ˆæ¨ç†è¿‡ç¨‹ï¼‰ï¼Œè€ŒBLTçš„æ”¹è¿›ä¸»è¦åœ¨äºè¡¨ç¤ºå±‚é¢ã€‚å¯¹äºéœ€è¦æ˜ç¡®æ¨ç†æ­¥éª¤çš„ä»»åŠ¡ï¼ŒCoTæ–¹æ³•å¯èƒ½æ›´æœ‰æ•ˆã€‚

ä¸è¿‡è¿™ä¸¤ç§æ–¹æ³•ä¹Ÿä¸æ˜¯äº’æ–¥çš„ï¼Œç†æƒ³æƒ…å†µä¸‹å¯ä»¥å°†å®ƒä»¬ç»“åˆï¼š
- ä½¿ç”¨BLTæä¾›æ›´å¥½çš„åº•å±‚è¡¨ç¤º
- ç”¨CoTå¾®è°ƒæ¥å¢å¼ºé«˜å±‚æ¨ç†èƒ½åŠ›

<p><strong>åŸæ–‡ï¼šA crucial improvement of BLT over tokenization-based models is that redefines the trade off between the vocabulary size and compute. In standard llms, increasing the size of the vocabulary means larger tokens on average and therefore fewer steps for the model but also larger output dimension for the final projection layer of the model. 
</strong></p>

**é—®é¢˜3:** è¯è¡¨å˜å¤§ä»£è¡¨çš„FFNå±‚è®¡ç®—å¤æ‚åº¦å¢å¤§ï¼Œä½†ä¹ŸåŒæ—¶å¯¼è‡´å•tokenå®¹çº³çš„bytesæ›´å¤š?

**å›å¤3:** 
æ ¹æ®æ–‡ä¸­æ‰€è¿°ï¼Œè¿™ç¡®å®æ˜¯ä¸€ä¸ªæƒè¡¡ã€‚åœ¨æ ‡å‡†çš„å¤§è¯­è¨€æ¨¡å‹ä¸­ï¼Œå¢å¤§è¯è¡¨ä¼šå¸¦æ¥ä¸¤ä¸ªç›¸åçš„æ•ˆæœï¼šè¯è¡¨å˜å¤§ä¼šä½¿å¾—æ¯ä¸ªtokenå¹³å‡èƒ½åŒ…å«æ›´å¤šçš„å­—èŠ‚ï¼Œä»è€Œå‡å°‘æ¨¡å‹éœ€è¦å¤„ç†çš„æ­¥éª¤ï¼Œä½†åŒæ—¶ä¹Ÿä¼šå¢å¤§æœ€ç»ˆæŠ•å½±å±‚çš„è¾“å‡ºç»´åº¦ï¼Œå¢åŠ è®¡ç®—å¼€é”€ï¼Œæ­£æ˜¯ç”±äºè¿™ç§æƒè¡¡çš„å­˜åœ¨ï¼ŒåŸºäºåˆ†è¯(tokenization)çš„æ–¹æ³•åœ¨è°ƒæ•´tokenå¤§å°å’Œæ¨ç†æˆæœ¬æ–¹é¢çš„çµæ´»æ€§å¾ˆæœ‰é™ã€‚æ¯”å¦‚æ–‡ä¸­æåˆ°ï¼ŒLlama 3ç›¸æ¯”Llama 2çš„è¯è¡¨å¤§å°å¢åŠ äº†4å€ï¼Œä½†å¹³å‡tokenå¤§å°åªä»3.7å­—èŠ‚å¢åŠ åˆ°4.4å­—èŠ‚ã€‚

**é—®é¢˜4:** BLTèƒ½å¦åº”ç”¨äºä¸­æ–‡ï¼Ÿ

**å›å¤4:** 
BLTä½œä¸ºä¸€ä¸ªåŸºäºbyteçº§åˆ«çš„æ¨¡å‹åº”è¯¥å¯ä»¥ç›´æ¥åº”ç”¨äºä¸­æ–‡,ä½†éœ€è¦æ³¨æ„ä»¥ä¸‹å‡ ç‚¹:
1. ä¸­æ–‡å­—ç¬¦çš„UTF-8ç¼–ç ç‰¹ç‚¹:
   - æ¯ä¸ªæ±‰å­—é€šå¸¸ä½¿ç”¨3ä¸ªbytesæ¥ç¼–ç 
   - ä¼šå¯¼è‡´åºåˆ—é•¿åº¦æ¯”è‹±æ–‡ASCIIæ–‡æœ¬é•¿3å€å·¦å³
   - patch sizeçš„è®¾ç½®å¯èƒ½éœ€è¦ç›¸åº”è°ƒæ•´ä»¥é€‚åº”è¿™ä¸ªç‰¹ç‚¹

2. åˆ†ç»„ç­–ç•¥çš„è°ƒæ•´:
ä¸­æ–‡æ²¡æœ‰ç©ºæ ¼åˆ†è¯,ä¸èƒ½ä½¿ç”¨whitespace-based patchingï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨ä»¥ä¸‹å‡ ç§patchingæ–¹å¼:
   - Entropy-based patching: åŸºäºæ¨¡å‹é¢„æµ‹çš„æ¡ä»¶ç†µæ¥ç¡®å®šè¾¹ç•Œ
   - Identity patching: ä¸å¯¹åºåˆ—è¿›è¡Œä»»ä½•é¢„å…ˆåˆ‡åˆ†
   - é’ˆå¯¹ä¸­æ–‡ä¼˜åŒ–çš„patchingæ–¹æ³•,æ¯”å¦‚åŸºäºå­—çš„è¾¹ç•Œ

3. æ½œåœ¨ä¼˜åŠ¿:
   - å¯ä»¥é¿å…ä¸­æ–‡åˆ†è¯çš„é—®é¢˜
   - èƒ½å¤Ÿå¤„ç†ä¸­è‹±æ··åˆæ–‡æœ¬
   - å¯¹ç½•è§å­—ç¬¦æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›

4. éœ€è¦éªŒè¯çš„æ–¹é¢:
   - compression rateåœ¨ä¸­æ–‡ä¸Šçš„æ•ˆæœ
   - æ¨¡å‹åœ¨ä¸­æ–‡è¯­æ–™ä¸Šçš„æ€§èƒ½è¡¨ç°
   - æ˜¯å¦éœ€è¦é’ˆå¯¹ä¸­æ–‡ç‰¹ç‚¹è¿›è¡Œç‰¹æ®Šçš„è¶…å‚æ•°è°ƒæ•´

è™½ç„¶ç†è®ºä¸ŠBLTèƒ½å¤„ç†ä¸­æ–‡,ä½†ç›®å‰è®ºæ–‡æ²¡æœ‰åœ¨ä¸­æ–‡æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœã€‚è¦ç¡®å®šå…¶åœ¨ä¸­æ–‡ä¸Šçš„å®é™…æ•ˆæœ,è¿˜éœ€è¦è¿›è¡Œä¸“é—¨çš„å®éªŒéªŒè¯ã€‚

<p><strong>åŸæ–‡ï¼šThus patching cannot assume access to future bytes in order to choose how to segment the byte sequence. 
Formally, a patching scheme $$f_p$$ satisfies the property of incremental patching if it satisfies:</strong>

$$
f_p(ğ’™_{<i})=f_p(ğ’™)_{<i}
$$ 

**é—®é¢˜5:** è¿™ä¸ªå…¬å¼å¦‚ä½•ç†è§£ï¼Ÿ

**å›å¤5:** 
å…¬å¼ $$f_p(ğ’™_{<i})=f_p(ğ’™)_{<i}$$ æ˜¯å¢é‡åˆ†å—(incremental patching)çš„å½¢å¼åŒ–å®šä¹‰ã€‚
è®©æˆ‘æ¥è§£é‡Šè¿™ä¸ªå…¬å¼çš„å«ä¹‰ï¼š
- $$f_p(ğ’™_{<i})$$ è¡¨ç¤ºå¯¹åºåˆ—xçš„å‰iä¸ªå­—èŠ‚è¿›è¡Œåˆ†å—
- $$f_p(ğ’™)_{<i}$$ è¡¨ç¤ºå…ˆå¯¹æ•´ä¸ªåºåˆ—xè¿›è¡Œåˆ†å—ï¼Œç„¶ååªå–å‰iä¸ªä½ç½®çš„ç»“æœ
- ç­‰å¼è¡¨æ˜è¿™ä¸¤ç§æ–¹å¼å¿…é¡»å¾—åˆ°ç›¸åŒçš„ç»“æœ
- è¿™ä¸ªå…¬å¼çš„å®é™…æ„ä¹‰æ˜¯ï¼šå½“æ¨¡å‹åœ¨ç”Ÿæˆæ–‡æœ¬æ—¶ï¼Œåªæ ¹æ®å·²ç»çœ‹åˆ°çš„å­—èŠ‚æ¥å†³å®šåˆ†å—è¾¹ç•Œçš„ä½ç½®ï¼Œè€Œä¸”è¿™ä¸ªå†³å®šä¸ä¼šå› ä¸ºåé¢å‡ºç°çš„å†…å®¹è€Œæ”¹å˜ã€‚è¿™ç¡®ä¿äº†æ¨¡å‹å¯ä»¥åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å®æ—¶ã€ç¨³å®šåœ°è¿›è¡Œåˆ†å—å†³ç­–ã€‚

</details>

---

### **BLTæ¶æ„ï¼šä¸‰æ¨¡å—äº¤å“æ›²**

![Figure 2 BLT comprises three modules, a lightweight Local Encoder that encodes input bytes into patch representations, a computationally expensive Latent Transformer over patch representations, and a lightweight Local Decoder to decode the next patch of bytes. BLT incorporates byte n-gram embeddings and a cross-attention mechanism to maximize information flow between the Latent Transformer and the byte-level modules (Figure 5). Unlike fixed-vocabulary tokenization, BLT dynamically groups bytes into patches preserving access to the byte-level information.](https://arxiv.org/html/2412.09871v1/x3.png)


**BLTçš„æ ¸å¿ƒç”±ä¸‰ä¸ªäº’è”æ¨¡å—ç»„æˆï¼š**

  - æœ¬åœ°ç¼–ç å™¨ï¼ˆLocal Encoderï¼‰ï¼š ä½¿ç”¨å“ˆå¸Œn-gramåµŒå…¥å’Œäº¤å‰æ³¨æ„åŠ›å±‚å¯¹åŸå§‹å­—èŠ‚è¿›è¡Œç¼–ç ï¼Œç”Ÿæˆè¡¥ä¸è¡¨ç¤ºã€‚

     ![Figure 5 The local encoder uses a cross-attention block with patch representations as queries, and byte representations as keys/values to encode byte representations into patch representations. The local decoder uses a similar block but with the roles reversed i.e. byte representations are now the queries and patch representations are the keys/values. Here we use Cross-Attn k= 2.](https://arxiv.org/html/2412.09871v1/x5.png)

  - æ½œåœ¨å˜æ¢å™¨ï¼ˆLatent Transformerï¼‰ï¼š å¤„ç†è¡¥ä¸è¡¨ç¤ºçš„å…¨å±€æ¨¡å‹ï¼Œé‡‡ç”¨å—å› æœæ³¨æ„åŠ›æœºåˆ¶é«˜æ•ˆå¤„ç†ä¸Šä¸‹æ–‡ä¾èµ–ã€‚

  - æœ¬åœ°è§£ç å™¨ï¼ˆLocal Decoderï¼‰ï¼š å°†è¡¥ä¸è¡¨ç¤ºè§£ç å›å­—èŠ‚ï¼Œæ”¯æŒå‡†ç¡®çš„ç”Ÿæˆå’Œç†è§£ã€‚

è¿™ç§æ¨¡å—åŒ–è®¾è®¡ç¡®ä¿äº†å­—èŠ‚å’Œè¡¥ä¸ä¿¡æ¯çš„æ— ç¼é›†æˆï¼Œæ—¢ä¿ç•™äº†åŸå§‹æ•°æ®çš„ä¸°å¯Œæ€§ï¼Œåˆä¼˜åŒ–äº†è®¡ç®—æ•ˆç‡ã€‚

<details class="toggle-header">

 <p><strong>åŸæ–‡ï¼šThis model consumes the bulk of the flops during pre-training as well as inference, and thus, choosing when to invoke it allows us to control and vary the amount of compute expended for different portions of the input and output as a function of input/output complexity.</strong></p>

**é—®é¢˜1:** è¿™æ®µè¯å¦‚ä½•ç†è§£ï¼ŸBLTå¦‚ä½•ä¾æ®è¾“å…¥/è¾“å‡ºå¤æ‚åº¦æ¥è°ƒæ•´å¯¹ä¸åŒè¾“å…¥å’Œè¾“å‡ºéƒ¨åˆ†æ‰€åˆ†é…çš„è®¡ç®—èµ„æºï¼Ÿ

**å›å¤1:** 

BLTé€šè¿‡åŠ¨æ€åˆ†é…è®¡ç®—èµ„æºçš„è®¾è®¡æ¥å®ç°é«˜æ•ˆçš„è®¡ç®—é‡åˆ†é…ã€‚è®©æˆ‘è§£é‡Šä¸€ä¸‹å…·ä½“æœºåˆ¶ï¼š

1. ç†µé¢„æµ‹é©±åŠ¨çš„åŠ¨æ€åˆ†é…ï¼š
   - BLTä½¿ç”¨ä¸€ä¸ªå°å‹å­—èŠ‚çº§è¯­è¨€æ¨¡å‹æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªå­—èŠ‚çš„ç†µ - å½“ç†µå€¼è¶…è¿‡è®¾å®šé˜ˆå€¼æ—¶ï¼Œä¼šæ ‡è®°ä¸ºä¸€ä¸ªpatchçš„è¾¹ç•Œ - ç†µå€¼é«˜æ„å‘³ç€ä¸‹ä¸€ä¸ªå­—èŠ‚é¢„æµ‹çš„ä¸ç¡®å®šæ€§å¤§ï¼Œéœ€è¦æ›´å¤šè®¡ç®—èµ„æº

2. å…·ä½“å·¥ä½œæµç¨‹ï¼š
   - å¯¹äºç†µå€¼ä½çš„éƒ¨åˆ†(å¦‚å•è¯æœ«å°¾çš„å­—æ¯)ï¼Œåªä½¿ç”¨è½»é‡çº§çš„Local Decoder - å½“é‡åˆ°ç†µå€¼é«˜çš„éƒ¨åˆ†(å¦‚å¥å­å¼€å¤´æˆ–ç”Ÿåƒ»è¯)ï¼Œæ‰è°ƒç”¨è®¡ç®—é‡å¤§çš„Latent Global Transformer

3. å®é™…ä¾‹å­ï¼š
   - é¢„æµ‹"computer"è¿™ä¸ªè¯æ—¶ï¼š - "c": ç†µå€¼é«˜ â†’ è°ƒç”¨Global Transformer - "omputer": ç†µå€¼ä½ â†’ åªç”¨Local Decoder

4. ä¼˜åŠ¿ï¼š
   - é¿å…å¯¹ç®€å•é¢„æµ‹ä½¿ç”¨å¤§é‡è®¡ç®—èµ„æº
   - å°†è®¡ç®—èƒ½åŠ›é›†ä¸­åœ¨çœŸæ­£éœ€è¦çš„å¤æ‚é¢„æµ‹ä¸Š
   - å®ç°äº†è®¡ç®—èµ„æºçš„è‡ªé€‚åº”åˆ†é…

5. æ•ˆæœï¼š
   - åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶é™ä½äº†50%çš„æ¨ç†è®¡ç®—é‡
   - ç‰¹åˆ«é€‚åˆå¤„ç†é•¿åºåˆ—è¾“å…¥ï¼Œå› ä¸ºå¯ä»¥åŠ¨æ€è·³è¿‡ç®€å•éƒ¨åˆ†

è¿™ç§è®¾è®¡è®©æ¨¡å‹èƒ½å¤Ÿ"æ™ºèƒ½åœ°"åˆ†é…è®¡ç®—èµ„æºï¼Œå°†å¤§éƒ¨åˆ†è®¡ç®—åŠ›ç”¨åœ¨çœŸæ­£éœ€è¦çš„åœ°æ–¹ï¼Œä»è€Œæé«˜æ•´ä½“æ•ˆç‡ã€‚

<p><strong>åŸæ–‡ï¼šThe transformer layers use a local block causal attention mask; each byte attends to a fixed window of $w_â„°$ preceding bytes that in general can cross the dynamic patch boundaries but can not cross document boundaries. </strong></p>

**é—®é¢˜2:** byte encoder hidden statesæ˜¯å¦ä¼šä½œä¸ºLatent Global Transformerçš„è¾“å…¥ï¼Ÿ

**å›å¤2:** 

byte encoder hidden statesä¸ä¼šç›´æ¥å‚ä¸åˆ°Latent Global Transformerçš„è®¡ç®—ä¸­ã€‚è®©æˆ‘è§£é‡Šå…¶ä¿¡æ¯æµå‘ï¼š
- ä¿¡æ¯æµå‘è¿‡ç¨‹ï¼š
    byte encoder hidden states â†’ patch cross attention â†’ patch representations â†’ Latent Global Transformer

- æ•°æ®è½¬æ¢ï¼š 
    1. byte embeddingsè½¬æ¢ä¸ºhidden states
        - h_init = LocalTransformer(byte_embeddings)
    2. cross attentionå°†hidden statesèšåˆä¸ºpatchè¡¨ç¤º
        - h_down = StridedConv(h_init)  
        è¿™é‡Œpatch representationsæ‰ä¼šè¾“å…¥Latent Global Transformer
    3. Latent Global Transformeråªå¤„ç†patchçº§åˆ«çš„è¡¨ç¤º
        - h_down' = GlobalTransformer(h_down)

- å…³é”®ç‚¹ï¼š
    1. byte encoder hidden statesçš„ä¿¡æ¯è¢«å‹ç¼©å’Œèšåˆåˆ°patch representationsä¸­
    2. Latent Global Transformeråªæ¥æ”¶å’Œå¤„ç†patch levelçš„è¡¨ç¤º
    3. åŸå§‹çš„byte levelä¿¡æ¯ä¸ä¼šç›´æ¥å‚ä¸Globalæ¨¡å‹çš„è®¡ç®—

- è¿™ç§è®¾è®¡çš„å¥½å¤„æ˜¯ï¼š
    1. é™ä½äº†åºåˆ—é•¿åº¦ï¼Œæé«˜è®¡ç®—æ•ˆç‡
    2. ä¿æŒäº†å±‚æ¬¡åŒ–çš„ç‰¹å¾è¡¨ç¤º
    3. é€šè¿‡cross attentionæœºåˆ¶ä¿ç•™å¿…è¦çš„å­—èŠ‚çº§ä¿¡æ¯

æ‰€ä»¥byte encoder hidden statesä¸»è¦ä½œç”¨æ˜¯å¸®åŠ©æ„å»ºpatchè¡¨ç¤ºï¼Œè€Œä¸æ˜¯ç›´æ¥å‚ä¸Latent Global Transformerçš„è®¡ç®—ã€‚

<p><strong>åŸæ–‡ï¼šThe local decoder predicts a sequence of raw bytes, as a function of previously decoded bytes, and thus, takes as input the hidden representations produced by the local encoder for the byte-sequence.
</strong></p>

**é—®é¢˜3:** local decoderæ˜¯å¦ä½¿ç”¨äº†local encoderçš„è¾“å‡ºä½œä¸ºè¾“å…¥ï¼Ÿ

**å›å¤3:** 

æœ¬åœ°è§£ç å™¨ï¼ˆlocal decoderï¼‰ä½¿ç”¨äº†æœ¬åœ°ç¼–ç å™¨ï¼ˆlocal encoderï¼‰äº§ç”Ÿçš„éšè—è¡¨ç¤ºï¼ˆhidden representationsï¼‰ä½œä¸ºè¾“å…¥ã€‚å…·ä½“æ¥è¯´ï¼Œè§£ç å™¨åœ¨é¢„æµ‹åŸå§‹å­—èŠ‚åºåˆ—æ—¶ï¼Œä¼šåˆ©ç”¨ä¹‹å‰å·²è§£ç çš„å­—èŠ‚ä¿¡æ¯ï¼Œå¹¶å°†ç¼–ç å™¨ç”Ÿæˆçš„éšè—è¡¨ç¤ºä½œä¸ºè¾“å…¥æ¥è¾…åŠ©è§£ç è¿‡ç¨‹ã€‚

<p><strong>åŸæ–‡ï¼šwhere once again, $$W_k$$, $$W_v$$ are key/value projection matrices that operate on a linear transformation and split operation $$ğ’Ÿ_C$$, applied to the final patch representations $$o_j$$ from the global model, $$W_q$$ is a query projection matrices operating on byte representations $$d_{lâˆ’1}$$ from the previous decoder transformer layer (or $$h_{l_â„°}$$ for the first layer), and $W_o$ is the output projection matrix, thus making $$Bâˆˆâ„^{h_ğ’ŸÃ—n_b}$$, where $n_b$ is the number of output bytes.</strong></p>

**é—®é¢˜4:** split operation $$ğ’Ÿ_C$$ å…·ä½“æ˜¯å¦‚ä½•å®ç°çš„ï¼Ÿ

**å›å¤4:** 
ä»ä»£ç ä¸­çœ‹åˆ°splitå®é™…ä¸Šæ˜¯é€šè¿‡reshapeå®ç°çš„ï¼Œä»¥ä¸‹è¯¦ç»†åˆ†æä¸€ä¸‹å…·ä½“å®ç°ï¼š

åœ¨LocalDecoderç±»ä¸­ï¼š

    def forward(self, tokens, embeds, patch_embeds, ...):
        # ...
        if self.patch_embedding_projection is not None:
            # 1. é¦–å…ˆé€šè¿‡çº¿æ€§æŠ•å½±
            patch_embeds = self.patch_embedding_projection(patch_embeds)
            
            # 2. å¦‚æœå­˜åœ¨cross_attn_k,è¿›è¡Œreshapeæ“ä½œ
            if self.cross_attn_k is not None:
                # è¿™é‡Œå®ç°äº†splitæ“ä½œ
                patch_embeds = patch_embeds.reshape(
                    bs,  # batch_size 
                    patch_embeds.shape[1] * self.cross_attn_k,  
                    # æ–°çš„åºåˆ—é•¿åº¦
                    self.dim  # æ¯ä¸ªheadçš„ç»´åº¦
                )

è®©æˆ‘è§£é‡Šè¿™ä¸ªreshapeæ“ä½œçš„ç»†èŠ‚ï¼š

1. patch_embedding_projectionçš„ä½œç”¨ï¼š

        def _create_patch_projection(self, args):
            if not self._should_create_patch_projection(args):
                return None
            
            # è¾“å‡ºç»´åº¦æ˜¯token_emb_dimä¹˜ä»¥cross_attn_k
            output_dim = args.dim_token_emb * (self.cross_attn_k or 1)
            
            return nn.Linear(
                in_features=args.dim_patch_emb,
                out_features=output_dim,
                bias=False,
            )

2. reshapeçš„è¿‡ç¨‹ï¼š
   - è¾“å…¥å½¢çŠ¶: [batch_size, seq_len, dim_patch_emb]
   - çº¿æ€§æŠ•å½±å: [batch_size, seq_len, dim_token_emb * cross_attn_k]
   - reshapeå: [batch_size, seq_len * cross_attn_k, dim_token_emb]

è¿™è¡¨æ˜ï¼š
  - splitä¸æ˜¯é€šè¿‡åˆ‡åˆ†æˆ–å¹³å‡æ“ä½œå®ç°çš„
  - è€Œæ˜¯å…ˆé€šè¿‡çº¿æ€§æŠ•å½±æ‰©å±•ç»´åº¦
  - ç„¶åé€šè¿‡reshapeé‡æ’è¿™äº›ç»´åº¦
  - cross_attn_kå†³å®šäº†è¦åˆ†æˆå¤šå°‘ä¸ªhead

è¿™ç§å®ç°æ–¹å¼çš„ä¼˜ç‚¹æ˜¯ï¼š
  - ä¿æŒäº†æ•°æ®çš„è¿ç»­æ€§
  - é¿å…äº†æ˜¾å¼çš„åˆ‡åˆ†æ“ä½œ
  - è®©æ¯ä¸ªheadèƒ½å¤Ÿå­¦ä¹ åˆ°ä¸åŒçš„ç‰¹å¾è¡¨ç¤º
  - è®¡ç®—æ•ˆç‡æ›´é«˜

è¿™ä¸è®ºæ–‡æè¿°çš„"maintain P_l as multiple heads"æ˜¯ä¸€è‡´çš„ï¼Œåªæ˜¯å®ç°æ–¹å¼æ›´åŠ ä¼˜é›…å’Œé«˜æ•ˆã€‚

**é—®é¢˜5:** è¿™é‡Œçº¿æ€§æŠ•å½±ï¼Œä¸ºä»€ä¹ˆä¸ç›´æ¥æŠ•å½±åˆ°reshapeåçš„ç»´åº¦å‘¢ï¼Ÿè€Œéœ€è¦å†è¿›è¡Œä¸€æ¬¡reshape?ï¼Ÿ

**å›å¤5:** 

è¿™æ˜¯ä¸€ä¸ªå¸¸è§çš„å®ç°æ¨¡å¼ï¼š
å…ˆç”¨çº¿æ€§å±‚å°† [batch, seq_len, dim\_patch\_emb] æŠ•å½±åˆ° [batch, seq_len, (cross\_attn\_k * dim\_token\_emb)]çš„â€œæ‰å¹³â€å½¢çŠ¶ï¼Œç„¶åå†ç”¨ reshape/view å¯¹æœ€åä¸€ç»´è¿›è¡Œæ‹†åˆ†æˆ–åˆå¹¶ï¼Œä»è€Œå¾—åˆ°æƒ³è¦çš„[batch, seq_len * cross\_attn\_k, dim\_token\_emb] æˆ– [batch, seq_len, cross\_attn\_k, dim\_token\_emb] ç­‰å¤šç»´ç»“æ„ã€‚
è¿™æ ·åšæœ€ä¸»è¦æ˜¯ä¸ºäº†éµå¾ª çº¿æ€§å±‚çš„æ ‡å‡†è¾“å…¥/è¾“å‡ºæ ¼å¼ ä»¥åŠ åœ¨åç»­é˜¶æ®µçµæ´»åœ°å¤„ç†æ‹†åˆ†å’Œå¹¶è¡Œç»´åº¦ï¼ˆæ¯”å¦‚å¤šå¤´ã€å¤šåˆ†å—ï¼‰ã€‚
åœ¨ PyTorch ä¸­ï¼Œä¸€ä¸ª nn.Linear(in_features, out_features) é€šå¸¸åªå¯¹æœ€åä¸€ç»´åšçº¿æ€§å˜æ¢ï¼Œåç»­å¦‚æœè¦è¿›ä¸€æ­¥æ‹†åˆ†æˆ–åˆå¹¶ï¼Œéœ€è¦æ˜¾å¼åœ°åšä¸€ä¸ª reshapeã€‚

**è¯¦ç»†è§£æ**

åœ¨è¿™æ®µä»£ç é‡Œï¼Œæ ¸å¿ƒæµç¨‹æ˜¯ï¼š

1.	çº¿æ€§æŠ•å½±ï¼ˆnn.Linearï¼‰ï¼š
   - å°†è¾“å…¥ä» [batch_size, patch_seq_len, dim\_patch\_emb] å½¢çŠ¶ï¼ŒæŠ•å½±æˆ [batch_size, patch_seq_len, cross\_attn\_k * dim\_token\_emb] ã€‚
   - ä»£ç é‡Œ out_features = dim_token_emb * cross_attn_kã€‚

2.	reshape æ“ä½œï¼š
   - æŠŠæŠ•å½±åçš„æœ€åä¸€ç»´ â€œæ‹†åˆ†â€/â€œæŠ˜å â€æˆæ–°çš„åºåˆ—é•¿åº¦å’Œæ–°ç‰¹å¾ç»´åº¦ï¼š

   [batch, patch_seq_len, (cross\_attn\_k * dim\_token\_emb)]
$$
\;\;\longrightarrow\;\;
$$
   [batch, (patch\_seq\_len * cross\_attn\_k), dim\_token\_emb]

   - æˆ–è€…åœ¨å…¶ä»–æƒ…å†µä¸‹ï¼Œä¹Ÿå¯ä»¥ reshape æˆ

   [batch, patch\_seq\_len, cross\_attn\_k, dim\_token\_emb]

ç­‰ç­‰ï¼Œä¸åŒé¡¹ç›®ä¸­å¯èƒ½æœ‰ä¸åŒçš„æ‹†åˆ†æ–¹å¼ã€‚


ä¹‹æ‰€ä»¥ ä¸ç›´æ¥ åœ¨çº¿æ€§å±‚ä¸­å°±è¾“å‡ºä¸‰ç»´ç”šè‡³å››ç»´çš„å½¢çŠ¶ï¼Œè€Œæ˜¯å…ˆè¾“å‡º 
[â€¦, cross\_attn\_k * dim\_token\_emb] 
å† reshapeï¼ŒèƒŒåæœ‰ä»¥ä¸‹å¸¸è§åŸå› ï¼š

1. PyTorch ä¸­ nn.Linear çš„â€œå¸¸è§„åšæ³•â€
    PyTorch (ä¹ƒè‡³å¤§å¤šæ•°æ·±åº¦å­¦ä¹ æ¡†æ¶) ä¸­çš„ nn.Linear(in_features, out_features)ï¼Œå…¶è¾“å…¥ä¸€èˆ¬æ˜¯ [batch\_dims, seq\_len, in\_features]ï¼Œç„¶åå°†æœ€åä¸€ç»´ in_features æŠ•å½±åˆ° out_featuresï¼Œè¾“å‡ºå½¢çŠ¶æ˜¯ [batch\_dims, seq\_len, out\_features]ã€‚
    å¦‚æœè¦è·å¾—æ›´é«˜ç»´åº¦çš„è¾“å‡ºï¼ˆä¾‹å¦‚ [..., cross_attn_k, dim_token_emb]ï¼‰ï¼Œå¾€å¾€éœ€è¦æ˜¾å¼åœ°åœ¨ç®—å®Œçº¿æ€§å±‚ä»¥åè¿›è¡Œ reshapeã€‚
	
2.	å®ç°ç®€å•ï¼Œå¯è¯»æ€§å’Œç»´æŠ¤æ€§æ›´å¥½
	â€¢	å…ˆå°†æœ€åä¸€ç»´ç»Ÿä¸€æŠ•å½±åˆ° out\_features = cross\_attn\_k \times dim\_token\_emb ï¼Œå†åœ¨åé¢å•ç‚¹åœ° reshapeï¼Œå®ç°é€»è¾‘æ›´ç›´è§‚ã€‚
	â€¢	å¦‚æœåç»­è¦æ”¹åŠ¨æ‹†åˆ†æ–¹å¼ï¼ˆä¾‹å¦‚å¤šå¤´ã€å¤šåˆ†å—ï¼‰ï¼Œåªéœ€è¦æ”¹åŠ¨åé¢ä¸€ä¸ª reshape çš„é€»è¾‘å³å¯ï¼Œè€Œä¸å¿…æ”¹åŠ¨ nn.Linear çš„åˆå§‹åŒ–ã€‚

3. åœ¨å¤šå¤´ã€å¤šè·¯åˆ†å—æ—¶çš„â€œé€šç”¨å¥—è·¯â€
	â€¢	å¾ˆå¤šå¤šå¤´æœºåˆ¶ï¼ˆmulti-head attentionï¼‰æˆ–å¤šåˆ†å—ï¼ˆmulti-groupï¼‰éƒ½ä¼šå…ˆæŠŠ hidden\_dim çº¿æ€§æŠ•å½±åˆ° (num_heads * head_dim)ï¼Œå†ç”¨ reshape/view/transpose å°†åºåˆ—ç»´åº¦æˆ–å¤´çš„ç»´åº¦æ‹†å‡ºæ¥ã€‚
	â€¢	è¿™æ˜¯ä¸ªæå…¶å¸¸è§çš„æ¨¡å¼ï¼Œç›®çš„æ˜¯åœ¨çŸ©é˜µä¹˜æ³•ä¸­ç»´æŒä¸€ä¸ªå¹²å‡€ä¸”é€šç”¨çš„ã€ŒäºŒç»´/ä¸‰ç»´è¾“å…¥è¾“å‡ºã€é€»è¾‘ï¼Œåé¢å†ç”¨ reshape æ¥åšçµæ´»çš„ç»´åº¦å¤„ç†ã€‚

4.	é¿å…å†™â€œè‡ªå®šä¹‰â€çº¿æ€§å±‚
    â€¢	å¦‚æœæƒ³è¦ä¸€æ¬¡æ€§è¾“å‡º [batch, seq\_len * cross\_attn\_k, dim\_token\_emb] é‚£å¯èƒ½å°±éœ€è¦è‡ªå·±å†™ä¸€ä¸ªå¸¦æœ‰è‡ªå®šä¹‰ broadcast/reshape çš„ Linearï¼Œä¼šè®©ä»£ç æ›´å¤æ‚ã€å¯è¯»æ€§æ›´å·®ã€‚
    â€¢	åœ¨å®é™…å·¥ç¨‹ä¸­ï¼Œå…ˆ nn.Linear(in_features, out_features)ï¼Œç„¶ååš view/reshape å‡ ä¹æ˜¯çº¦å®šä¿—æˆçš„åšæ³•ã€‚

ç±»æ¯”ï¼šå¤šå¤´æ³¨æ„åŠ›çš„æ‹†åˆ†

ä¸€ä¸ªå’Œè¿™é‡Œéå¸¸ç±»ä¼¼çš„å¸¸è§åœºæ™¯æ˜¯ å¤šå¤´è‡ªæ³¨æ„åŠ› (Multi-Head Self-Attention)ï¼š

1.	æˆ‘ä»¬é€šå¸¸å…ˆç”¨ nn.Linear(hidden_dim, num_heads * head_dim)
2.	å† reshape æˆ [batch, seq\_len, num\_heads, head\_dim]
3.	æœ€ç»ˆå† transpose ä¸€ä¸‹å¾—åˆ° [batch, num\_heads, seq\_len, head\_dim]


å®ƒçš„æ€æƒ³å’Œä½ çœ‹åˆ°çš„è¿™æ®µ BLT patch_embeds ä»£ç æ˜¯ä¸€æ ·çš„ï¼š
	â€¢	å…ˆå°†å€’æ•°ç¬¬äºŒä¸ªç»´åº¦æŠ•å½±åˆ°ä¸€ä¸ªæ‰å¹³çš„ num_heads * head_dimï¼Œ
	â€¢	å† reshape åˆ†æ‹†å‡º num_heads è¿™ä¸ªæ–°ç»´åº¦ã€‚


åœ¨ Byte/patch çº§åˆ«çš„ cross-attnã€åˆ†å—ã€æˆ– multi-group ç­‰åœºæ™¯ä¸‹ï¼Œä¹Ÿæ˜¯ä¸€æ¨¡ä¸€æ ·çš„å¥—è·¯ã€‚

å°ç»“ :â€œå…ˆçº¿æ€§æŠ•å½±ï¼Œå† reshapeâ€ æ˜¯ PyTorch ç­‰æ¡†æ¶é‡Œä¸»æµçš„ã€æƒ¯ç”¨çš„å®ç°æ–¹å¼ï¼Œä¹Ÿæ˜¯ä½¿ä»£ç æ›´é€šç”¨å’Œæ˜“ç»´æŠ¤çš„å¸¸è§å®è·µã€‚


ç®€è€Œè¨€ä¹‹ï¼Œnn.Linear() åªç®¡æŠŠç»´åº¦ä» [â€¦, in\_features] æŠ•å½±åˆ° [â€¦, out\_features]ï¼ŒæŠŠæ›´å¤šç»´åº¦æˆ–æ—¶åºé•¿åº¦çš„å¤„ç†é€»è¾‘ç•™ç»™äº†åç»­çš„ reshape/view/transposeï¼Œå› æ­¤å®ƒå…ˆè¾“å‡ºä¸€ä¸ªâ€œæ‰å¹³â€çš„ç»´åº¦ï¼Œå†å»åšæ‹†åˆ†ã€åˆå¹¶æˆ–å¹¿æ’­ï¼Œå°±èƒ½éå¸¸çµæ´»åœ°é€‚é…å¤šå¤´ã€å¤šåˆ†å—ã€ä»¥åŠå„ç§ä¸åŒä¸‹æ¸¸æ¨¡å—çš„éœ€æ±‚ã€‚

</details>

---
### å®éªŒè®¾ç½®æ€»ç»“

å®éªŒè®¾ç½®åŒ…æ‹¬é¢„è®­ç»ƒæ•°æ®é›†çš„é€‰æ‹©ã€ç†µæ¨¡å‹çš„è®­ç»ƒã€patchingç­–ç•¥çš„ä¼˜åŒ–ä»¥åŠFLOPsçš„è®¡ç®—æ–¹æ³•ã€‚é€šè¿‡è¿™äº›è®¾ç½®ï¼Œç¡®ä¿å®éªŒç»“æœçš„å¯æ¯”æ€§å’Œå¯é æ€§ã€‚

ä»¥ä¸‹æ˜¯å®éªŒè®¾ç½®çš„ç²¾åæ€»ç»“ï¼š

#### 1. **é¢„è®­ç»ƒæ•°æ®é›†**
   - **Llama 2 æ•°æ®é›†**ï¼šåŒ…å«2ä¸‡äº¿ä¸ªtokenï¼Œç”¨äºæ‰©å±•å®šå¾‹å®éªŒï¼Œç¡®å®šBLTçš„æœ€ä½³æ¶æ„é€‰æ‹©ã€‚
   - **BLT-1T æ•°æ®é›†**ï¼šåŒ…å«1ä¸‡äº¿ä¸ªtokenï¼Œç”¨äºä¸Llama 3è¿›è¡Œä¸‹æ¸¸ä»»åŠ¡å¯¹æ¯”çš„å®Œæ•´é¢„è®­ç»ƒã€‚
   - **æ•°æ®æ¥æº**ï¼šæ‰€æœ‰æ•°æ®å‡æ¥è‡ªå…¬å¼€æ¥æºï¼Œç¡®ä¿å®éªŒçš„é€æ˜æ€§å’Œå¯é‡å¤æ€§ã€‚

#### 2. **ç†µæ¨¡å‹**
   - **æ¨¡å‹æ¶æ„**ï¼šä½¿ç”¨100Må‚æ•°çš„å­—èŠ‚çº§è¯­è¨€æ¨¡å‹ï¼ŒåŸºäºTransformeræ¶æ„ï¼Œæ»‘åŠ¨çª—å£æ³¨æ„åŠ›ä¸º512å­—èŠ‚ã€‚
   - **åŠ¨æ€patching**ï¼šé€šè¿‡ç†µæ¨¡å‹é¢„æµ‹ä¸‹ä¸€å­—èŠ‚çš„ä¸ç¡®å®šæ€§ï¼ŒåŠ¨æ€è°ƒæ•´patchå¤§å°ï¼Œç¡®ä¿è®¡ç®—èµ„æºçš„é«˜æ•ˆåˆ†é…ã€‚

#### 3. **ç†µé˜ˆå€¼ä¸ä¸Šä¸‹æ–‡é•¿åº¦**
   - **åŠ¨æ€è°ƒæ•´**ï¼šæ ¹æ®ç†µé˜ˆå€¼åŠ¨æ€è°ƒæ•´patchå¤§å°ï¼Œç¡®ä¿åœ¨æ•°æ®å¤æ‚åº¦é«˜çš„åŒºåŸŸåˆ†é…æ›´å¤šè®¡ç®—èµ„æºã€‚
   - **ä¸Šä¸‹æ–‡é•¿åº¦å‡è¡¡**ï¼šé€šè¿‡è°ƒæ•´åºåˆ—é•¿åº¦ï¼Œç¡®ä¿ä¸åŒpatchå¤§å°çš„æ¨¡å‹åœ¨ç›¸åŒçš„å­—èŠ‚æ‰¹æ¬¡ä¸Šè¿›è¡Œè®­ç»ƒï¼Œé¿å…ä¸å…¬å¹³çš„æ¯”è¾ƒã€‚

#### 4. **FLOPsè®¡ç®—**
   - **è®¡ç®—æ–¹æ³•**ï¼šåŸºäºChinchillaçš„Transformer FLOPsè®¡ç®—æ–¹ç¨‹ï¼Œç»“åˆå±€éƒ¨ç¼–ç å™¨ã€å…¨å±€Transformerå’Œå±€éƒ¨è§£ç å™¨çš„FLOPsã€‚
   - **è¾“å…¥åµŒå…¥ä¼˜åŒ–**ï¼šå‡è®¾è¾“å…¥åµŒå…¥å±‚ä¸ºé«˜æ•ˆæŸ¥æ‰¾æ“ä½œï¼Œå‡å°‘è®¡ç®—å¼€é”€ã€‚

#### 5. **æ¯å­—èŠ‚æ¯”ç‰¹æ•°ï¼ˆBPBï¼‰**
   - **æ›¿ä»£å›°æƒ‘åº¦**ï¼šBPBä½œä¸ºtokenizerç‹¬ç«‹çš„æ€§èƒ½æŒ‡æ ‡ï¼Œç”¨äºæ¯”è¾ƒå­—èŠ‚çº§æ¨¡å‹å’Œtoken-basedæ¨¡å‹çš„è¡¨ç°ã€‚

#### 6. **Transformeræ¶æ„è¶…å‚æ•°**
   - **æ¶æ„ä¸€è‡´æ€§**ï¼šBLTçš„å±€éƒ¨å’Œå…¨å±€Transformerå‡éµå¾ªLlama 3çš„æ¶æ„ï¼Œä½¿ç”¨SwiGLUæ¿€æ´»å‡½æ•°ã€æ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆRoPEï¼‰å’ŒRMSNormå±‚å½’ä¸€åŒ–ã€‚
   - **æ³¨æ„åŠ›æœºåˆ¶**ï¼šä½¿ç”¨Flash Attentionä¼˜åŒ–è‡ªæ³¨æ„åŠ›å±‚ï¼ŒFlex Attentionä¼˜åŒ–äº¤å‰æ³¨æ„åŠ›å±‚ï¼Œæå‡è®­ç»ƒæ•ˆç‡ã€‚

#### 7. **BLTç‰¹å®šè¶…å‚æ•°**
   - **æ¨¡å‹è§„æ¨¡**ï¼šå®éªŒæ¶µç›–400Måˆ°8Bå‚æ•°çš„æ¨¡å‹ï¼ŒéªŒè¯BLTåœ¨ä¸åŒè§„æ¨¡ä¸‹çš„æ‰©å±•æ€§ã€‚
   - **ä¼˜åŒ–è®¾ç½®**ï¼šä½¿ç”¨AdamWä¼˜åŒ–å™¨ï¼Œå­¦ä¹ ç‡ä¸º4e-4ï¼Œç»“åˆçº¿æ€§é¢„çƒ­å’Œä½™å¼¦è¡°å‡è°ƒåº¦ï¼Œç¡®ä¿è®­ç»ƒç¨³å®šæ€§ã€‚

#### 8. **å®éªŒç›®æ ‡**
   - **æ€§èƒ½å¯¹æ¯”**ï¼šä¸Llama 3ç­‰token-basedæ¨¡å‹è¿›è¡Œå¯¹æ¯”ï¼ŒéªŒè¯BLTåœ¨ç›¸åŒè®¡ç®—é¢„ç®—ä¸‹çš„æ€§èƒ½ä¼˜åŠ¿ã€‚
   - **é²æ£’æ€§éªŒè¯**ï¼šé€šè¿‡å™ªå£°è¾“å…¥å’Œå­—ç¬¦çº§ä»»åŠ¡ï¼ŒéªŒè¯BLTåœ¨å¤„ç†å¤æ‚æ•°æ®æ—¶çš„é²æ£’æ€§ã€‚
   - **æ‰©å±•æ€§éªŒè¯**ï¼šé€šè¿‡ä¸åŒpatchå¤§å°å’Œæ¨¡å‹è§„æ¨¡çš„å®éªŒï¼ŒéªŒè¯BLTåœ¨æ‰©å±•æ€§ä¸Šçš„ä¼˜åŠ¿ã€‚

#### æ€»ç»“
BLTçš„å®éªŒè®¾ç½®é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æ•°æ®é›†ã€åŠ¨æ€patchingç­–ç•¥å’Œé«˜æ•ˆçš„FLOPsè®¡ç®—ï¼ŒéªŒè¯äº†å…¶åœ¨æ€§èƒ½ã€æ•ˆç‡å’Œé²æ£’æ€§ä¸Šçš„ä¼˜åŠ¿ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBLTä¸ä»…èƒ½å¤ŸåŒ¹é…token-basedæ¨¡å‹çš„æ€§èƒ½ï¼Œè¿˜åœ¨æ¨ç†æ•ˆç‡å’Œæ‰©å±•æ€§ä¸Šå±•ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸ºæœªæ¥è¯­è¨€å»ºæ¨¡æä¾›äº†æ–°çš„æ–¹å‘ã€‚


<details class="toggle-header">

<p><strong>åŸæ–‡ï¼šWe carefully design controlled experiments to compare BLT with tokenization based models with particular attention to not give BLT any advantages from possibly using longer sequence contexts.
</strong></p>

**é—®é¢˜1:** ä¸ºä»€ä¹ˆè¦æ§åˆ¶sequence length?è¿™ä¸ªå¯¹è¯•éªŒæœ‰ä»€ä¹ˆå½±å“ï¼Ÿï¼Ÿ

**å›å¤1:** 

åœ¨åŸæ–‡ä¸­ï¼Œä½œè€…å¤šæ¬¡å¼ºè°ƒè¦ä¿è¯ BLT å’ŒåŸºäº token çš„æ¨¡å‹åœ¨å®éªŒä¸­â€œçœ‹åˆ°â€æˆ–å¤„ç†åˆ°çš„åºåˆ—ï¼ˆæ–‡æœ¬ï¼‰é•¿åº¦å¤§ä½“ä¸€è‡´ï¼Œå³â€œä¸è®© BLT å› ä¸ºèƒ½å¤„ç†æ›´é•¿ä¸Šä¸‹æ–‡è€Œé¢å¤–è·åˆ©â€ã€‚è¿™ä¸€ç‚¹ä¹‹æ‰€ä»¥é‡è¦ï¼Œä¸»è¦åŸºäºä»¥ä¸‹è€ƒè™‘ï¼š

1. Byte çº§æ¨¡å‹ä¸ Token çº§æ¨¡å‹å¤©ç„¶åºåˆ—é•¿åº¦ä¸åŒ

	â€¢	å¯¹äºä¼ ç»Ÿçš„åŸºäº subword token çš„æ¨¡å‹ï¼ˆå¦‚ BPEï¼‰ï¼Œä¸€ä¸ª token å¹³å‡å¯èƒ½å¯¹åº” 3~4 ä¸ªå­—èŠ‚ï¼›

	â€¢	è€Œåœ¨ BLT ä¸­ï¼Œæœ€åŸå§‹çš„è¾“å…¥æ˜¯æŒ‰å­—èŠ‚ï¼ˆbyteï¼‰çº§è¾“å…¥ï¼Œå¦‚æœä¸åŠ æ§åˆ¶ï¼ŒByte æ¨¡å‹åœ¨åŒæ ·çš„â€œæ­¥éª¤â€ï¼ˆæˆ–åŒæ ·çš„ batch sizeï¼‰ä¸‹å°±æœ‰å¯èƒ½çœ‹åˆ°æ›´å¤šå®é™…å­—ç¬¦ï¼Œä»è€Œè·å¾—â€œé¢å¤–çš„ä¿¡æ¯é‡â€æˆ–â€œæ›´é•¿çš„ä¸Šä¸‹æ–‡â€ã€‚

	â€¢	å› æ­¤ï¼Œå¦‚æœè®© Byte æ¨¡å‹ç›´æ¥å¤„ç†æ¯” BPE æ¨¡å‹æ›´å¤šçš„åŸå§‹å­—èŠ‚æ•°ï¼Œå¾ˆå¯èƒ½ä¼šé€ æˆå®éªŒå¯¹æ¯”ä¸å…¬å¹³ï¼Œéš¾ä»¥åˆ¤æ–­æ€§èƒ½æå‡ç©¶ç«Ÿæ¥è‡ªæ¶æ„æ”¹è¿›è¿˜æ˜¯æ¥è‡ªå¯è®¿é—®çš„ä¸Šä¸‹æ–‡å˜é•¿ã€‚

2. åºåˆ—é•¿åº¦ä¼šå½±å“æ¨¡å‹çš„è®¡ç®—é‡ä¸æ€§èƒ½

    â€¢	å¦‚æœ BLT çš„æ¯æ¬¡è®­ç»ƒæˆ–æ¨ç†éƒ½èƒ½â€œçœ‹åˆ°â€æ¯” BPE æ¨¡å‹æ›´é•¿çš„å®é™…æ–‡æœ¬åºåˆ—ï¼Œé‚£ä¹ˆæœ‰å¯èƒ½å¸¦æ¥æ›´å¥½çš„è¯­è¨€å»ºæ¨¡æ•ˆæœï¼Œä½†è¿™å¹¶éçº¯ç²¹ç”± BLT çš„â€œåŠ¨æ€åˆ†ç»„/patchingâ€è®¾è®¡å¸¦æ¥çš„ä¼˜åŠ¿ï¼Œè€Œæ˜¯å› ä¸ºé¢å¤–çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

    â€¢	å¦ä¸€æ–¹é¢ï¼Œåºåˆ—é•¿åº¦ä¹Ÿä¸è®¡ç®—æˆæœ¬ï¼ˆFLOPsï¼‰å¼ºç›¸å…³ï¼šåºåˆ—è¶Šé•¿ï¼Œéœ€è¦çš„ self-attention æˆ– feed-forward è®¡ç®—æ­¥æ•°å°±è¶Šå¤šã€‚å€˜è‹¥ BLT ä¸é™åˆ¶åºåˆ—é•¿åº¦ï¼Œè€Œ BPE çš„åºåˆ—é•¿åº¦è¢«é™åˆ¶ï¼Œé‚£ BLT å°±åœ¨ç®—åŠ›/æˆæœ¬ä¸æ•°æ®ä½¿ç”¨ä¸Šéƒ½å­˜åœ¨â€œä¸å¯¹ç­‰â€æ¡ä»¶ã€‚

3. æ§åˆ¶å˜é‡ä»¥æ’é™¤å¹²æ‰°å› ç´ ï¼Œçªå‡ºæ¨¡å‹æœ¬èº«çš„å¯¹æ¯”

    â€¢	åšå¯¹æ¯”å®éªŒæ—¶ï¼Œé€šå¸¸ä¼šé‡‡ç”¨â€œæ§åˆ¶å˜é‡æ³•â€å»å°½é‡æ’é™¤æ‰€æœ‰æ— å…³å› ç´ ï¼ˆå¦‚ä¸Šä¸‹æ–‡é•¿åº¦ã€èƒ½çœ‹åˆ°çš„å­—ç¬¦æ•°ç­‰ï¼‰ï¼Œä»è€ŒæŠŠç„¦ç‚¹é›†ä¸­åœ¨ BLT çš„â€œå¯åŠ¨æ€åˆ†ç»„â€ä»¥åŠâ€œå­—èŠ‚çº§å»ºæ¨¡â€æœ¬èº«çš„æ•ˆæœä¸Šã€‚

    â€¢	ä¸€æ—¦è®© BLT â€œçœ‹åˆ°â€æ›´å¤šçš„å­—èŠ‚ï¼Œæ— æ³•æ˜ç¡®åŒºåˆ†æ˜¯å› ä¸ºè§åˆ°æ›´å¤šæ–‡æœ¬é€ æˆçš„æ”¶ç›Šï¼Œè¿˜æ˜¯ç¡®å®æ¥è‡ªæ¶æ„æ”¹è¿›ï¼ˆä¾‹å¦‚ local encoderã€n-gram hash embeddingsã€cross-attentionã€dynamic patching ç­‰ï¼‰çš„æ”¶ç›Šã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œä¹‹æ‰€ä»¥åœ¨è®ºæ–‡ä¸­ä¸¥æ ¼æ§åˆ¶æˆ–å¯¹é½ Byte æ¨¡å‹ä¸åŸºäº Token çš„æ¨¡å‹çš„è¾“å…¥åºåˆ—é•¿åº¦ï¼Œæ˜¯ä¸ºäº†è®©äºŒè€…åœ¨â€œåŒæ ·çš„æ–‡æœ¬é•¿åº¦â€æˆ–â€œåŒæ ·çš„å¯è®¿é—®ä¿¡æ¯â€æ¡ä»¶ä¸‹è¿›è¡Œå¯¹æ¯”ï¼Œä»è€Œæ›´å‡†ç¡®åœ°è¯„ä¼° BLT æ¶æ„æœ¬èº«ï¼ˆå¦‚åŠ¨æ€ patchingã€Local Encoder/Decoder ç­‰ï¼‰çš„æ”¹è¿›å¸¦æ¥çš„æ”¶ç›Šï¼Œè€Œä¸æ˜¯è¢«é¢å¤–çš„å¯è§æ–‡æœ¬é•¿åº¦æ‰€â€œç¨€é‡Šâ€æˆ–â€œæ”¾å¤§â€ã€‚
åœ¨å®éªŒè®¾è®¡ä¸Šï¼Œè¿™ä¼šå½±å“åˆ°ï¼š

- è®­ç»ƒå’Œæ¨ç†çš„ batch size è®¾å®šï¼šéœ€è¦ç¡®ä¿ Byte æ¨¡å‹å’Œ Token æ¨¡å‹åœ¨ç›¸åŒ batch æƒ…å†µä¸‹ï¼Œå¤„ç†åˆ°çš„å®é™…å­—èŠ‚æ•°æˆ–æ–‡æœ¬è§„æ¨¡æ˜¯ç­‰ä»·çš„ã€‚
- ä¸Šä¸‹æ–‡çª—å£å¤§å°çš„é€‰æ‹©ï¼šByte æ¨¡å‹å› ä¸ºæ˜¯å­—èŠ‚çº§ï¼Œé»˜è®¤ä¼šæ¯” Token æ¨¡å‹æœ‰æ›´é•¿çš„æ½œåœ¨åºåˆ—ï¼Œå¦‚æœä¸æ§åˆ¶ï¼ŒByte æ¨¡å‹æ¯ä¸ªæ ·æœ¬å¯èƒ½çœ‹åˆ°æ›´å¤šä¸Šä¸‹æ–‡ã€‚
- FLOPsï¼ˆæµ®ç‚¹è¿ç®—é‡ï¼‰æˆ–å†…å­˜å ç”¨ï¼šè¾ƒé•¿åºåˆ—çš„æ³¨æ„åŠ›è®¡ç®—æ›´è´µï¼Œæ§åˆ¶åºåˆ—é•¿åº¦å¯ä»¥è®©æ¯”è¾ƒå˜å¾—æ›´å…¬å¹³ã€‚

åªæœ‰åœ¨è¿™æ ·â€œè¢«æ§åˆ¶â€çš„åœºæ™¯ä¸‹ï¼Œæ‰èƒ½å‡¸æ˜¾ BLT åœ¨åŒç­‰æ¡ä»¶ä¸‹ä¸ BPE ç±»æ¨¡å‹çš„çœŸå®å·®å¼‚ï¼Œå¹¶åšå‡ºæ›´å®¢è§‚çš„ç»“è®ºã€‚

**é—®é¢˜2:** å¯¹äºâ€œByte æ¨¡å‹å› ä¸ºæ˜¯å­—èŠ‚çº§ï¼Œé»˜è®¤ä¼šæ¯” Token æ¨¡å‹æœ‰æ›´é•¿çš„æ½œåœ¨åºåˆ—â€çš„ç†è§£ï¼Œæ„æ€æ˜¯ï¼šç›¸åŒçš„ä¸€å¥è¯ï¼Œè½¬æ¢ä¸ºtokensçš„åºåˆ—é•¿åº¦æ¯”åŸºäºBLTæ¨¡å‹è½¬æ¢çš„å­—èŠ‚åºåˆ—è¦çŸ­ï¼Œæ˜¯å—ï¼Ÿ

**å›å¤2:** 

æ˜¯çš„ï¼Œé€šå¸¸æƒ…å†µä¸‹ï¼Œå¦‚æœæˆ‘ä»¬æŠŠåŒä¸€å¥è¯åˆ†åˆ«ç”¨
- å­—èŠ‚çº§ï¼ˆByte-levelï¼‰ çš„æ–¹å¼è¡¨ç¤º
- BPE/è¯ç‰‡ï¼ˆToken-basedï¼‰ çš„æ–¹å¼è¡¨ç¤º

é‚£ä¹ˆå­—èŠ‚åºåˆ—é€šå¸¸ä¼šæ›´é•¿ï¼ˆå› ä¸ºæ¯ä¸ªå­—ç¬¦éƒ½è¦ç”¨ä¸€ä¸ªæˆ–å¤šä¸ªå­—èŠ‚æ¥è¡¨ç¤ºï¼‰ï¼Œè€Œ BPE æ¨¡å‹åˆ™ä¼šæŠŠå¤šä¸ªå­—ç¬¦åˆå¹¶æˆä¸€ä¸ª tokenï¼Œå› æ­¤å¾—åˆ°çš„åºåˆ—ä¼šæ›´çŸ­ã€‚

ä¸¾ä¸ªç®€å•ä¾‹å­ï¼š â€œHello worldâ€

- ä»¥è‹±æ–‡ ASCII å­—èŠ‚æ–¹å¼è¡¨ç¤ºï¼šH e l l o  w o r l dï¼ˆå…± 11 ä¸ªå­—ç¬¦ï¼ŒåŒ…æ‹¬ç©ºæ ¼ï¼‰ï¼Œå®é™…ä¸Šè¿˜è¦ç®—ä¸Šç»“å°¾ç¬¦å·ã€æ¢è¡Œã€ç‰¹æ®Šæ ‡è®°ç­‰ï¼Œå¾€å¾€å­—èŠ‚æ•°ä¼šæ›´å¤šã€‚
- ç»è¿‡ BPE å¯èƒ½åªä¼šå¾—åˆ° 2~3 ä¸ª tokenï¼ˆæ¯”å¦‚ [Hello]\ [_world]ï¼‰ï¼Œå› æ­¤åºåˆ—é•¿åº¦è¾ƒçŸ­ã€‚


è¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆåœ¨ä¸åŠ æ§åˆ¶çš„æƒ…å†µä¸‹ï¼ŒByte-level æ¨¡å‹ä¼šâ€œçœ‹åˆ°â€æ›´é•¿çš„åºåˆ—ï¼šåŒæ ·çš„æ–‡æœ¬ï¼Œå­—èŠ‚åºåˆ—å¤©ç„¶åœ°æ¯” BPE åºåˆ—æ›´é•¿ã€‚ä¸ºäº†åšå…¬å¹³çš„å¯¹æ¯”ï¼Œè®ºæ–‡ä¸­å¾€å¾€ä¼šæ§åˆ¶æˆ–å¯¹é½äºŒè€…â€œå®é™…å¤„ç†åˆ°çš„æ–‡æœ¬é•¿åº¦â€æˆ–â€œå¤„ç†åˆ°çš„å­—èŠ‚æ€»æ•°â€ï¼Œé¿å…è®© Byte-level æ¨¡å‹å› ä¸ºå¤„ç†äº†æ›´å¤šåŸå§‹å­—èŠ‚è€Œé¢å¤–è·åˆ©ã€‚

**é—®é¢˜3:** æ–‡ç« ä¸­æ˜¯æ§åˆ¶åŸºäºç›¸åŒå­—èŠ‚çš„context lengthå¯¹BLTä¸token based modelè¿›è¡Œå¯¹æ¯”è¯•éªŒçš„å—ï¼Ÿ

**å›å¤3:** 
æ˜¯çš„ï¼Œä»–ä»¬ä¸»è¦æ˜¯æŒ‰â€œå­—èŠ‚ï¼ˆbytesï¼‰æ€»æ•°â€æ¥æ§åˆ¶ä¸Šä¸‹æ–‡çš„é•¿åº¦ï¼Œä»è€Œä¿è¯åœ¨åŒä¸€æ¬¡è®­ç»ƒæˆ–æ¨ç†ä¸­ï¼ŒBLT å’ŒåŸºäº Token çš„æ¨¡å‹åœ¨â€œèƒ½çœ‹åˆ°çš„åŸå§‹å­—èŠ‚æ€»é‡â€ä¸Šæ˜¯ä¸€è‡´çš„ã€‚è¿™æ ·å°±é¿å…äº† BLT å› ä¸ºé»˜è®¤å­—èŠ‚åºåˆ—æ›´é•¿è€Œâ€œé¢å¤–è·åˆ©â€çš„é—®é¢˜ã€‚

å…·ä½“æ¥è¯´ï¼Œè®ºæ–‡ä¸­æåˆ°çš„åšæ³•å¤§è‡´åŒ…å«ä»¥ä¸‹è¦ç‚¹ï¼š

1.	å›ºå®šæ¯ä¸ªæ‰¹æ¬¡(batch)çš„å­—èŠ‚æ€»æ•°:
    ä¾‹å¦‚ï¼Œåœ¨å¤„ç† Llama 2 æ•°æ®é›†æ—¶ï¼Œä½œè€…è®©æ¯ä¸ªåºåˆ—çš„å¹³å‡é•¿åº¦ä¸º 8k å­—èŠ‚ï¼›åœ¨å¤„ç† BLT-1T æ•°æ®é›†æ—¶ï¼Œä»–ä»¬è®©æ¯ä¸ªåºåˆ—çš„å¹³å‡é•¿åº¦ä¸º 16k å­—èŠ‚ã€‚ 
    å¦‚æ­¤ä¸€æ¥ï¼Œæ— è®ºæ˜¯ BPE æ¨¡å‹è¿˜æ˜¯ BLTï¼Œåœ¨åŒä¸€ä¸ª batch ä¸­ï¼Œéƒ½åªèƒ½â€œçœ‹åˆ°â€å¤§çº¦ç›¸åŒæ•°é‡çš„åŸå§‹å­—èŠ‚ã€‚

2.  å¯¹åº”åœ°è°ƒæ•´ BPE æ¨¡å‹çš„åºåˆ—é•¿åº¦:
    å¯¹äº BPE/token æ¨¡å‹ï¼Œå¦‚æœä¸€è¡Œæ–‡æœ¬æœ‰ 8k å­—èŠ‚ï¼ŒToken åŒ–ä»¥åå¯èƒ½åªæœ‰å‡ åƒä¸ª tokenï¼ˆç”šè‡³æ›´å°‘ï¼‰ï¼Œä½†æ€»çš„â€œåŸå§‹å­—èŠ‚æ•°â€ä»ç„¶æ§åˆ¶åœ¨ 8kã€‚
    BLT åˆ™ç›´æ¥å¤„ç† 8k å­—èŠ‚çš„åºåˆ—ï¼ˆå†è¿›ä¸€æ­¥åˆ†æˆ dynamic patchï¼‰ï¼Œä¸¤è¾¹åœ¨å­—èŠ‚é‡ä¸Šæ˜¯å¯¹é½çš„ã€‚

3.	é¿å…è®© BLT â€œçœ‹åˆ°â€æ¯” BPE æ¨¡å‹æ›´å¤šçš„å®é™…ä¿¡æ¯:
    è‹¥ä¸æ§åˆ¶ï¼ŒBLT å¯èƒ½ä¼šç›´æ¥æ‹¿åˆ°æ›´å¤šçš„ä¸Šä¸‹æ–‡ï¼ˆå› ä¸ºå®ƒæŒ‰å­—èŠ‚è¯»å…¥ï¼‰ï¼Œè€Œ BPE æ¨¡å‹åœ¨åŒæ ·çš„æ–‡æœ¬ä¸‹å¾€å¾€åªå¯¹åº”æ›´çŸ­çš„ token åºåˆ—ã€‚
    é€šè¿‡å›ºå®šâ€œå­—èŠ‚æ€»æ•°â€çš„åšæ³•ï¼Œå¯ä»¥æ’é™¤â€œè°çœ‹äº†æ›´å¤šæ–‡æœ¬â€è¿™ä¸ªå› ç´ ï¼ŒæŠŠå®éªŒå¯¹æ¯”çš„é‡ç‚¹æ”¾åœ¨äº†ã€ŒæŒ‰å­—èŠ‚ vs. æŒ‰ tokenã€ä»¥åŠã€ŒåŠ¨æ€åˆ†ç»„ vs. å›ºå®šåˆ†è¯ã€æœ¬èº«çš„å·®å¼‚ä¸Šã€‚

4.	Flopsï¼ˆè®¡ç®—æˆæœ¬ï¼‰å’Œä¸Šä¸‹æ–‡é•¿åº¦ä¹Ÿå› æ­¤å¾—åˆ°å¯å¯¹é½çš„è¡¡é‡:
    ä½œè€…è¿˜è¿›ä¸€æ­¥åšäº†æ¨ç†/è®­ç»ƒ Flops çš„ç²¾ç»†ä¼°ç®—ï¼Œå¹¶åœ¨åŒç­‰â€œå¯è§å­—èŠ‚æ•°â€æˆ–åŒç­‰è®­ç»ƒé¢„ç®—æ—¶æ¯”è¾ƒæ€§èƒ½ï¼Œä¿è¯äº†å¯¹æ¯”çš„å…¬å¹³æ€§ã€‚


å› æ­¤ï¼Œæ–‡ä¸­æ‰€è°“çš„â€œcarefully design controlled experimentsâ€å°±æ˜¯åœ¨ä¿è¯åŒç­‰å­—èŠ‚ä¸Šä¸‹æ–‡åŠç›¸åŒè®­ç»ƒ/æ¨ç† FLOPs é¢„ç®—çš„æƒ…å†µä¸‹ï¼Œå¯¹ BLT ä¸åŸºäº token çš„æ¨¡å‹è¿›è¡Œä¸€ç³»åˆ—å®éªŒï¼Œä»¥æ’é™¤åºåˆ—é•¿åº¦æˆ–æ•°æ®è§„æ¨¡ä¸åŒå¸¦æ¥çš„å¹²æ‰°ï¼Œè®©è¯»è€…èƒ½æ›´å¥½åœ°ç†è§£ BLT æ¶æ„æœ¬èº«çš„ä¼˜åŠ£åŠ¿ã€‚

</details>

---

### **Key Findings and Contributions**



BLTçš„è´¡çŒ®é‡æ–°å®šä¹‰äº†LLMçš„èŒƒå¼ï¼š

- Flop Efficiencyï¼ˆæµ®ç‚¹æ“ä½œæ•ˆç‡ï¼‰ï¼š åœ¨è®­ç»ƒæ€§èƒ½ä¸ŠåŒ¹é…äº†å¦‚LLaMA 3ç­‰åŸºäºtokenizedçš„æ¨¡å‹ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†æ¨ç†æˆæœ¬ã€‚

- Robustnessï¼ˆé²æ£’æ€§ï¼‰ï¼š å±•ç¤ºäº†å¯¹å™ªå£°è¾“å…¥çš„å¼ºå¤§æŠµæŠ—èƒ½åŠ›å’Œå¢å¼ºçš„å­—ç¬¦çº§ç†è§£èƒ½åŠ›ï¼Œåœ¨æ­£å­—æ³•çŸ¥è¯†å’Œä½èµ„æºæœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸­è¡¨ç°å“è¶Šã€‚

- Scaling Potentialï¼ˆæ‰©å±•æ½œåŠ›ï¼‰ï¼š è§£é”äº†LLMæ‰©å±•çš„æ–°ç»´åº¦ï¼Œå…è®¸è¡¥ä¸å’Œæ¨¡å‹å°ºå¯¸çš„åŒæ—¶å¢é•¿ã€‚

---

### **Empirical Insights: BLT in Action**

BLT's superiority is evident in its performance on standard benchmarks and specific tasks. For instance:

- **Noise Robustness:** Outperforms token-based models in handling noisy inputs, such as character-level distortions.
- **Orthographic Tasks:** Excels in character manipulation tasks, leveraging direct byte-level access.
- **Translation:** Achieves higher scores in low-resource language translations, demonstrating its adaptability to diverse linguistic contexts.

---

### **Beyond Tokenization: The Road Ahead**

The introduction of BLT signals a transformative era in LLM development. By prioritizing efficiency, robustness, and scalability, it paves the way for more inclusive, adaptable, and powerful language models. Researchers and developers can access BLTâ€™s training and inference code on [GitHub](https://github.com/facebookresearch/blt).

For correspondence or collaboration inquiries, contact Artidoro Pagnoni or Srinivasan Iyer via the provided emails.

---

**Conclusion**
BLT is more than an architecture; it is a statement against the limitations of traditional tokenization. By redefining how LLMs process language, it lays the groundwork for future innovations that prioritize flexibility, efficiency, and fairness in AI-driven communication. The age of byte-level language modeling has truly begun.
BLTä¸ä»…ä»…æ˜¯ä¸€ç§æ¶æ„ï¼›è¿™æ˜¯å¯¹ä¼ ç»Ÿæ ‡è®°åŒ–ï¼ˆTokenizeï¼‰å±€é™æ€§çš„ä¸€ç§å£°æ˜ã€‚é€šè¿‡é‡æ–°å®šä¹‰LLMè‡ªç„¶è¯­è¨€å¤„ç†è¯­è¨€çš„æ–¹å¼ï¼Œå®ƒä¸ºæœªæ¥çš„åˆ›æ–°å¥ å®šäº†åŸºç¡€ï¼Œåœ¨äººå·¥æ™ºèƒ½é©±åŠ¨çš„äº¤æµä¸­ï¼Œä¼˜å…ˆè€ƒè™‘çµæ´»æ€§ã€æ•ˆç‡å’Œå…¬å¹³æ€§ã€‚å­—èŠ‚çº§è¯­è¨€å»ºæ¨¡çš„æ—¶ä»£å·²ç»çœŸæ­£å¼€å§‹ã€‚