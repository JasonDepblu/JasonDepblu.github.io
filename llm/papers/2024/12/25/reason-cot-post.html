<body class="post-layout">
<!--     Header -->
    <header class="header">
      <div class="container">
        <span class="logo">🌟 Jason's Blog</span>
        <nav class="nav">
          <ul>
            
              <li><a href="/myblog/">Posts</a></li>
            
              <li><a href="/myblog/archive/">Archive</a></li>
            
              <li><a href="/myblog/search/">Search</a></li>
            
              <li><a href="/myblog/tags/">Tags</a></li>
            
              <li><a href="/myblog/about/">About</a></li>
            
              <li><a href="/myblog/contact/">Contact</a></li>
            
          </ul>
        </nav>
      </div>
    </header>

    <article>
        <h1>Coconut：在连续潜在空间中的大语言模型推理范式研究</h1>
        <div class="post-meta">
          <span class="post-date">Date: December 25, 2024</span> |
          <span class="reading-time">Estimated Reading Time: 15 min</span> |
          <span class="post-author">Author: Jason Deng</span>
        </div>
        <main class="container">
            <p><strong>Author:</strong> Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, Yuandong Tian</p>
<ol>
  <li>FAIR at Meta</li>
  <li>UC San Diego<br />
∗ Work done at Meta</li>
</ol>

<p><strong>URL:</strong> <a href="https://arxiv.org/html/2412.06769v2">https://arxiv.org/html/2412.06769v2</a><br />
<strong>Title:</strong> <em>Training Large Language Models to Reason in a Continuous Latent Space</em></p>

<hr />

<h2 id="一思维链cot推理系统性综述">一、思维链(CoT)推理：系统性综述</h2>

<h3 id="1-方法分类">1. 方法分类</h3>

<h4 id="11-提示工程">1.1 提示工程</h4>

<ul>
  <li>
    <p><strong>Jason Wei 等 (2022)：</strong><br />
通过设计思维链提示来引导大语言模型提供完整的推理路径，然后再生成最终答案，在复杂任务中显著提升了性能。</p>
  </li>
  <li>
    <p><strong>Tushar Khot 等 (2022)：</strong><br />
提出分解提示法，将复杂问题分解为多个子问题，逐步求解。</p>
  </li>
  <li>
    <p><strong>Denny Zhou 等 (2022)：</strong><br />
开发了由简至繁提示法，首先生成子问题，然后按序求解，以捕捉更深层的依赖关系。</p>
  </li>
</ul>

<h4 id="12-监督式微调">1.2 监督式微调</h4>

<ul>
  <li>
    <p><strong>Xiang Yue 等 (2023)：</strong><br />
在 Mammoth 项目中引入混合指令调优，提升了数学推理的泛化能力和鲁棒性。</p>
  </li>
  <li>
    <p><strong>Longhui Yu 等 (2023)：</strong><br />
Metamath 项目利用众包或自生成数据进行监督式微调，在数学任务中实现了更强的泛化能力。</p>
  </li>
</ul>

<h4 id="13-强化学习">1.3 强化学习</h4>

<ul>
  <li>
    <p><strong>Alex Havrilla 等 (2024)：</strong><br />
开创性地将强化学习应用于教授大语言模型 CoT 推理。</p>
  </li>
  <li>
    <p><strong>Shibo Hao 等 (2024)：</strong><br />
将推理视为结合世界模型的规划过程，增强了动态情境推理能力。</p>
  </li>
  <li>
    <p><strong>Zhihong Shao 等 (2024)：</strong><br />
DeepSeekMath 应用强化学习来克服高难度数学场景的局限性。</p>
  </li>
</ul>

<h4 id="14-token-分析">1.4 Token 分析</h4>

<ul>
  <li><strong>Aman Madaan 和 Amir Yazdanbakhsh (2022)：</strong><br />
研究了“符号”、“模式”和“文本” token 在引导高效简洁的 CoT 生成中的作用。</li>
</ul>

<h4 id="15-理论分析">1.5 理论分析</h4>

<ul>
  <li>
    <p><strong>Guhao Feng 等 (2023)：</strong><br />
从理论角度阐述了 CoT 如何增强模型的表达能力和深度。</p>
  </li>
  <li>
    <p><strong>William Merrill 和 Ashish Sabharwal (2023)：</strong><br />
研究了引入 CoT 的 Transformer 的扩展表征能力。</p>
  </li>
  <li>
    <p><strong>Zhiyuan Li 等 (2024)：</strong><br />
证明 CoT 帮助 Transformer 解决本质上的顺序性任务，使模型能够“深化”并逐步推理。</p>
  </li>
</ul>

<h3 id="2-关键挑战与改进方向">2. 关键挑战与改进方向</h3>

<h4 id="21-规划与搜索问题">2.1 规划与搜索问题</h4>

<ul>
  <li>
    <p><strong>Yann LeCun (2022)：</strong><br />
探讨了“单路径自回归生成”在复杂任务中的局限性。</p>
  </li>
  <li>
    <p><strong>Yuxi Xie 等 (2023)：</strong><br />
引入自评估引导集束搜索，使模型能在搜索过程中自我评估不同分支的质量，提高推理准确性。</p>
  </li>
  <li>
    <p><strong>Shunyu Yao 等 (2023)：</strong><br />
提出思维树 (Tree-of-Thoughts, ToT)，通过树搜索过程显式探索和重访多个推理分支。</p>
  </li>
</ul>

<hr />

<h2 id="二大语言模型中的潜在推理隐式推理的新视角">二、大语言模型中的潜在推理：隐式推理的新视角</h2>

<h3 id="1-定义和核心现象">1. 定义和核心现象</h3>

<h4 id="11-隐藏计算研究">1.1 隐藏计算研究</h4>

<ul>
  <li>
    <p><strong>Sohee Yang 等 (2024)：</strong><br />
研究大语言模型是否在多跳推理任务中隐式编码中间步骤。</p>
  </li>
  <li>
    <p><strong>Eden Biran 等 (2024)：</strong><br />
探讨多跳推理中“延迟决策”带来的局限性。</p>
  </li>
  <li>
    <p><strong>Yuval Shalev 等 (2024)：</strong><br />
在多跳任务中发现了隐式并行推理过程的证据。</p>
  </li>
</ul>

<h4 id="12-cot-中的不一致性">1.2 CoT 中的“不一致性”</h4>

<ul>
  <li>
    <p><strong>Boshi Wang 等 (2022)：</strong><br />
实证研究揭示模型生成的“链条”常常偏离实际内部计算，暴露出“表层-深层不匹配”现象。</p>
  </li>
  <li>
    <p><strong>Miles Turpin 等 (2024)：</strong><br />
强调了显式解释与真实推理路径之间的差异，引发了对安全性和可信度的思考。</p>
  </li>
</ul>

<h3 id="2-增强潜在推理">2. 增强潜在推理</h3>

<h4 id="21-额外-token-扩展">2.1 额外 Token 扩展</h4>

<ul>
  <li>
    <p><strong>Sachin Goyal 等 (2023)：</strong><br />
主张使用 <code class="language-plaintext highlighter-rouge">&lt;pause&gt;</code> 等特殊 token 来鼓励模型“思考后回应”，提升推理表现。</p>
  </li>
  <li>
    <p><strong>Jacob Pfau 等 (2024)：</strong><br />
发现在部分并行任务中插入“…”等填充 token 能带来性能提升。</p>
  </li>
</ul>

<h4 id="22-规划-token-预测">2.2 规划 Token 预测</h4>

<ul>
  <li><strong>Xinyi Wang 等 (2023)：</strong><br />
用规划 token 引导模型生成更有结构的推理链。</li>
</ul>

<h4 id="23-知识蒸馏">2.3 知识蒸馏</h4>

<ul>
  <li>
    <p><strong>Yuntian Deng 等 (2023 &amp; 2024)：</strong><br />
提出 iCoT（隐式思维链），通过知识蒸馏压缩推理步骤，在不需要显式输出的情况下内化 CoT。</p>
  </li>
  <li>
    <p><strong>Ping Yu 等 (2024)：</strong><br />
将显式的“系统 2”推理蒸馏为“系统 1”内部表征，降低推理成本。</p>
  </li>
</ul>

<h3 id="3-方法扩展与优化">3. 方法扩展与优化</h3>

<ul>
  <li><strong>循环 Transformer：</strong>
    <ul>
      <li><strong>Angeliki Giannou 等 (2023) 和 Ying Fan 等 (2024)：</strong><br />
引入循环 Transformer 用于迭代自处理，支持算法任务和长度泛化。这与 Coconut 的状态反馈“递归”推理机制相呼应。</li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="三与-coconut-的整合连接显式和潜在推理">三、与 Coconut 的整合：连接显式和潜在推理</h2>

<p>根据选定研究，神经影像学实验表明：在人类执行各类推理任务时，主要负责语言理解和生成的大脑语言网络区域往往并不高度活跃。基于 Coconut（连续思维链）的方法，不仅可以部分映射这种认知现象，也在推理效率上有显著优势。Coconut 通过在连续潜在空间中进行推理，而非依赖传统在自然语言空间中逐步生成推理步骤的思维链方法，引入了一个新的推理范式。</p>

<p>具体来说，Coconut 利用语言模型的最终隐藏状态作为“连续思维”的表征，直接将其作为后续步骤的输入，而不是将其解码为 token。主要变化和优势包括：</p>

<p><img src="/myblog/assets/images/Figure%201%20A%20comparison%20of%20Chain%20of%20Continuous%20Thought%20(Coconut)%20with%20Chain-of-Thought%20(CoT).png" alt="Comparison" /><br />
<em>Figure 1: A comparison of Chain of Continuous Thought (Coconut) with Chain-of-Thought (CoT).</em></p>

<h3 id="推理空间的转变">推理空间的转变</h3>
<ul>
  <li><strong>传统 CoT：</strong><br />
依赖自然语言逐步生成推理，在复杂任务中容易导致效率低下和表达限制。</li>
  <li><strong>Coconut：</strong><br />
在潜在空间中运作，更灵活高效，不受语言约束。</li>
</ul>

<h3 id="增强的推理能力">增强的推理能力</h3>
<ul>
  <li><strong>多路径推理：</strong><br />
在连续思维中可同时编码多个潜在的下一步，提升需要广泛搜索和回溯的任务表现。</li>
  <li><strong>降低错误传播：</strong><br />
避免每步显式语言输出，在潜在空间中优化推理路径，减少错误的累积。</li>
</ul>

<h3 id="效率提升">效率提升</h3>
<ul>
  <li><strong>减少 Token 生成：</strong><br />
以更少的步骤完成任务，显著减少 Token 生成量。</li>
  <li><strong>潜在空间优化：</strong><br />
直接使用隐藏状态进行推理，降低解码和编码开销。</li>
</ul>

<h3 id="训练与优化">训练与优化</h3>
<ul>
  <li><strong>多阶段训练：</strong><br />
逐步将显式 CoT 转化为连续思维，提高推理能力与训练效率。</li>
</ul>

<p>在多个推理任务（尤其是需要复杂规划和搜索的 ProsQA 等场景）中，Coconut 相比传统 CoT 展现出更高的准确率和更少的 Token 生成量。</p>

<p>相关研究佐证：</p>

<ul>
  <li><strong>Amalric 和 Dehaene (2019)：</strong><br />
研究非语言区域在逻辑推理任务中的作用。</li>
  <li><strong>Monti 等 (2007, 2009, 2012)：</strong><br />
多项研究强调语言处理与推理的解离。</li>
  <li><strong>Fedorenko 等 (2011)：</strong><br />
研究发现语言区域在推理过程中的参与度有限，提示了特定领域网络的重要性。</li>
</ul>

<hr />

<h2 id="四coconut-的关键特性与机制">四、Coconut 的关键特性与机制</h2>

<h3 id="1-从显式生成到隐式思考">1. 从“显式生成”到“隐式思考”</h3>

<p>与 CoT 的自回归显式搜索（如 ToT）不同，Coconut 能在连续潜在空间中并行保留多个候选路径，与递归推理和内化策略相匹配。它的训练细节进一步凸显了其独特方法：</p>

<p><img src="/myblog/assets/images/Figure%202%20Training%20procedure%20of%20Chain%20of%20Continuous%20Thought%20(Coconut).png" alt="Comparison" /><br />
<em>Figure 2: Training procedure of Chain of Continuous Thought (Coconut).</em></p>

<ol>
  <li>
    <p><strong>可微分性与反向传播</strong><br />
Coconut 的连续思维完全面向可微分，支持通过反向传播进行优化，学习更高效的连续推理表征。</p>
  </li>
  <li>
    <p><strong>前向传递</strong><br />
若当前训练阶段设置了 (n) 个潜在思维，需执行 (n+1) 次前向传递。每次生成新的潜在思维，最后一次传递用于处理剩余文本序列。</p>
  </li>
  <li>
    <p><strong>损失计算</strong><br />
在每次传递里生成新的潜在思维；最后一次前向传递处理任务输出并计算损失。</p>
  </li>
  <li>
    <p><strong>顺序与并行化挑战</strong><br />
KV 缓存可减轻部分重复计算，但因为每次传递依赖前一次结果，无法完全并行化。</p>
  </li>
  <li>
    <p><strong>未来优化方向</strong><br />
提升训练效率仍是研究重点，力求在不损害推理能力的前提下降低计算资源需求。</p>
  </li>
</ol>

<h3 id="2-连接表层语言和潜在推理">2. 连接“表层语言”和“潜在推理”</h3>

<p>Coconut 减少了对自然语言形式生成的依赖，避免了显式 CoT 输出中常见的不一致性。</p>

<h3 id="3-重新思考规划与搜索">3. 重新思考“规划与搜索”</h3>

<p>Coconut 的连续推理过程可被类比为“搜索树”而非线性推理链。它可在潜在空间中保留并评价多条分支，同时基于搜索价值动态调度：</p>

<ol>
  <li><strong>连续思维与搜索树类比</strong>
    <ul>
      <li>在搜索树中，每个节点代表一个潜在推理状态，分支代表可能的发展路径。</li>
      <li>Coconut 可同时对多个路径并行评分并进行剪枝。</li>
    </ul>
  </li>
  <li>
    <p><strong>示例（图 7）</strong><br />
若根节点为 Alex，第一步可探索 <code class="language-plaintext highlighter-rouge">{lempus, sterpus, zhorpus, grimpus}</code>；若选择 <code class="language-plaintext highlighter-rouge">lempus</code> 分支，下一步则从孙节点展开。Coconut 能自动评估哪些分支更有前景。</p>
  </li>
  <li><strong>与标准 BFS 的区别</strong>
    <ul>
      <li><strong>BFS：</strong> 统一广度探索前沿节点。</li>
      <li><strong>Coconut：</strong> 动态优先级评估，在潜在空间评估每个分支的“价值”，更灵活高效。</li>
    </ul>
  </li>
  <li><strong>语言空间解码</strong><br />
经过连续思维后，若返回语言空间，模型会解码出可能的推理路径。根据概率分布，可进一步分析其偏好及下一步选择。</li>
</ol>

<p><strong>总结：</strong><br />
Coconut 的优势在于隐式评估与动态调度，使其在需要广泛搜索与规划的复杂任务中具备更高效的推理能力。</p>

<h3 id="4-与多阶段训练和自监督的协同效应">4. 与多阶段训练和自监督的协同效应</h3>

<ol>
  <li><strong>多阶段训练与分解目标</strong>
    <ul>
      <li>通过多阶段课程将训练目标分解并逐步复杂化，Coconut 在复杂推理任务中表现更佳。</li>
      <li><code class="language-plaintext highlighter-rouge">&lt;pause&gt;</code> token 也可结合类似思路在部分场景中获得增益。</li>
    </ul>
  </li>
  <li><strong>与 iCoT 的比较</strong>
    <ul>
      <li>iCoT（隐式思维链）也使用多阶段训练，但在“计划移除”等细粒度操作上有所差异。</li>
      <li>二者方法或可结合，进一步强化模型在潜在空间的推理能力。</li>
    </ul>
  </li>
  <li><strong>对监督信号的依赖</strong>
    <ul>
      <li>目前 Coconut 训练时依赖显式思维链数据，将其转化为潜在推理目标。</li>
      <li>作者提出未来可探索不依赖显式语言链监督的潜在推理策略，使模型更具通用性。</li>
    </ul>
  </li>
</ol>

<h4 id="混合数据策略">混合数据策略</h4>

<p>在 Coconut 的多阶段训练中，以 (p = 0.3) 的概率将早期阶段数据混入当前训练阶段，有效防止模型遗忘之前学到的知识：</p>

<ul>
  <li>
    <p><strong>连续思维插入</strong><br />
中间阶段的隐层表征（连续嵌入或“思维”）以一定概率被混入后续训练目标中，保证连续性和统一性。</p>
  </li>
  <li>
    <p><strong>目标选择</strong><br />
这些隐层数据能作为额外监督信号，帮助模型在潜在空间和语言空间间更好地衔接。</p>
  </li>
  <li>
    <p><strong>防止遗忘</strong><br />
混合数据策略可最大程度保留早期训练内容，避免在后续阶段产生“灾难性遗忘”。</p>
  </li>
</ul>

<p>实验显示，该方法能在多阶段复杂推理任务中取得良好平衡，在潜在（隐层）与显式（文本）推理间自由切换。</p>

<p><img src="/myblog/assets/images/Table1_Results_on_three_datasets(GSM8l_ProntoQA_ProsQA).png" alt="Comparison" /><br />
<em>Table 1: Results on three datasets (GSM8l, ProntoQA, ProsQA).</em></p>

<hr />

<h2 id="五展望未来推理框架中的多范式集成">五、展望：未来推理框架中的多范式集成</h2>

<h3 id="1-跨范式融合">1. 跨范式融合</h3>

<p>将 <code class="language-plaintext highlighter-rouge">&lt;pause&gt;</code> token、规划 token 与 Coconut 结合使用，可兼顾显式可解释性与隐式并行性，适配更复杂的推理任务。</p>

<h3 id="2-自动化搜索与人工干预">2. 自动化搜索与人工干预</h3>

<p>将 Coconut 的潜在空间推理嵌入到 ToT 框架中可有效提升搜索效率与准确性，也为人工干预保留了接口。</p>

<h3 id="3-通用预训练重新设计">3. 通用预训练重新设计</h3>

<p>若在基础模型（如循环 Transformer）中融入“递归”或“连续推理”机制，可能在推理能力上带来跨越式提升。</p>

<hr />

<h2 id="六结论">六、结论</h2>

<p>本文通过回顾思维链（CoT）在提示工程、监督式微调、强化学习与理论分析等方面的最新发展，说明了显式推理链在诸多任务中的优势与不足。潜在推理的出现进一步挖掘了内化多跳推理与规划的潜力。</p>

<p>Coconut 将这两者加以整合，通过多阶段训练与潜在状态反馈，在连续空间中完成推理和规划，不再局限于单一路径的自回归生成方式。在大量需要规划、回溯的复杂任务中，Coconut 展现了突出的灵活性与效率。</p>

<p>未来，Coconut、iCoT、<code class="language-plaintext highlighter-rouge">&lt;pause&gt;</code> token 与循环 Transformer 等技术有望通过更大规模、更多样化的验证不断进化，逐渐弥合“语言空间推理”与“内部模型推理”之间的鸿沟，向真正的灵活、多路径的自主思考推进。</p>

        </main>
        
        <div class="tags">
            标签:
            
            <span class="tag">reason</span>
            
            <span class="tag">CoT</span>
            
        </div>
        
    </article>
    <!-- Footer -->
    <footer class="footer">
      <p>&copy; 2024 Jason'Blog</p>
    </footer>
</body>


<!--<link rel="stylesheet" href="/myblog/assets/main.css">-->
<link rel="stylesheet" href="/myblog/assets/css/custom.css">